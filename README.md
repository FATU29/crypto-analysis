# B√°o C√°o ƒê·ªì √Ån Web N√¢ng Cao: Ph√¢n T√≠ch Ki·∫øn Tr√∫c H·ªá Th·ªëng Ph√¢n T√°n

## T√åNH HU·ªêNG 1: SCALING WEBSOCKET CHO 1000+ CLIENT K·∫æT N·ªêI ƒê·ªíNG TH·ªúI

### Ph√¢n T√≠ch B·∫£n Ch·∫•t V·∫•n ƒê·ªÅ

Trong ·ª©ng d·ª•ng ph√¢n t√≠ch gi√° ti·ªÅn m√£ h√≥a, ng∆∞·ªùi d√πng k·∫øt n·ªëi WebSocket ƒë·ªÉ nh·∫≠n c·∫≠p nh·∫≠t gi√° theo th·ªùi gian th·ª±c t·ª´ c√°c s√†n giao d·ªãch (Binance, Kraken, Coinbase). Khi s·ªë l∆∞·ª£ng kh√°ch h√†ng tƒÉng t·ª´ 100 l√™n 1000+ ng∆∞·ªùi k·∫øt n·ªëi ƒë·ªìng th·ªùi, m·ªôt server duy nh·∫•t kh√¥ng th·ªÉ x·ª≠ l√Ω ƒë∆∞·ª£c:

- **Gi·ªõi h·∫°n t√†i nguy√™n**: M·ªói WebSocket connection ti√™u t·ªën ~1-2MB b·ªô nh·ªõ. 1000 connections = ~1-2GB RAM cho c√°c socket objects.
- **V·∫•n ƒë·ªÅ broadcast**: Khi nh·∫≠n d·ªØ li·ªáu gi√° t·ª´ Binance, server ph·∫£i g·ª≠i l·∫°i ƒë·∫øn t·∫•t c·∫£ 1000 client. N·∫øu server x·ª≠ l√Ω tu·∫ßn t·ª±, s·∫Ω b·ªã ngh·∫Ωn.
- **Single Point of Failure**: N·∫øu server b·ªã crash, t·∫•t c·∫£ 1000 client m·∫•t k·∫øt n·ªëi v√† ph·∫£i reconnect t·ª´ ƒë·∫ßu, g√¢y "thundering herd" (b√£o request).
- **Kh√¥ng th·ªÉ scale horizontally**: N·∫øu th√™m server th·ª© 2, client c·ªßa server 1 kh√¥ng th·ªÉ communicate ƒë∆∞·ª£c v·ªõi client c·ªßa server 2 (ch√∫ng n·∫±m trong memory ri√™ng bi·ªát).

![WebSocket Problem Analysis](diagrams_images/websocket_problem.png)

### Gi·∫£i Ph√°p 1: Sticky Session v·ªõi IP Hash Load Balancer

**C∆° ch·∫ø ho·∫°t ƒë·ªông:**
S·ª≠ d·ª•ng Nginx ho·∫∑c HAProxy v·ªõi IP Hash strategy. Khi client A (IP: 203.0.113.5) k·∫øt n·ªëi l·∫ßn ƒë·∫ßu, load balancer hash IP n√†y ƒë·ªÉ map ƒë·∫øn server 1. M·ªçi request ti·∫øp theo t·ª´ c√πng IP s·∫Ω lu√¥n route t·ªõi server 1. V√¨ v·∫≠y WebSocket connection ƒë∆∞·ª£c "d√≠nh" v√†o server n√†y su·ªët v√≤ng ƒë·ªùi.

V·ªõi 3 server:

- Server 1: Client A, B, C, ... (~333 clients)
- Server 2: Client D, E, F, ... (~333 clients)
- Server 3: Client G, H, I, ... (~334 clients)

Khi nh·∫≠n d·ªØ li·ªáu gi√° t·ª´ Binance, m·ªói server independently broadcast t·ªõi nh√≥m client c·ªßa m√¨nh, gi·∫£m t·∫£i t·ª´ 1000 xu·ªëng ~333 tr√™n m·ªói server.

**T·∫°i sao ph·∫£i d√πng c√°ch n√†y?**

**L√Ω do 1: B·∫£o to√†n tr·∫°ng th√°i connection d·ªÖ d√†ng**

- WebSocket l√† stateful protocol: m·ªói connection c√≥ buffer ri√™ng, session context, authentication state. N·∫øu client k·∫øt n·ªëi t·ªõi server kh√°c, ph·∫£i thi·∫øt l·∫≠p l·∫°i t·ª´ ƒë·∫ßu.
- Sticky session ƒë∆°n gi·∫£n: kh√¥ng c·∫ßn shared state database, kh√¥ng c·∫ßn synchronize gi·ªØa c√°c server. M·ªói server qu·∫£n l√Ω ri√™ng nh√≥m client c·ªßa m√¨nh.
- IP Hash l√† deterministic: c√πng IP lu√¥n hash c√πng server, n√™n n·∫øu client reconnect (m·∫•t signal WiFi r·ªìi l·∫°i k·∫øt n·ªëi), s·∫Ω v·ªÅ server c≈© ngay l·∫≠p t·ª©c.

**L√Ω do 2: Tr√°nh "Thundering Herd" problem trong reconnection**

- N·∫øu d√πng round-robin (load balancer ph√¢n ph·ªëi ng·∫´u nhi√™n), khi c√≥ s·ª± c·ªë, t·∫•t c·∫£ 1000 client reconnect c√πng l√∫c c√≥ th·ªÉ map t·ªõi server ng·∫´u nhi√™n.
- Khi ƒë√≥, n·∫øu c√≥ 200 client reconnect t·ªõi server 1 (thay v√¨ 333 c≈©), server s·∫Ω b·ªã shock t·ª´ spike traffic.
- IP Hash gi·ªØ nguy√™n mapping: 333 client c·ªßa server 1 lu√¥n reconnect v·ªÅ server 1, n√™n traffic ·ªïn ƒë·ªãnh, kh√¥ng c√≥ spike.

**L√Ω do 3: Chi ph√≠ network overhead th·∫•p**

- Sticky session kh√¥ng c·∫ßn g·ª≠i message qua network gi·ªØa c√°c server (kh√¥ng c·∫ßn message broker).
- So v·ªõi Redis Pub/Sub solution, kh√¥ng ph·∫£i ƒë·∫©y m·ªói price update t·ªõi Redis broker, r·ªìi Redis broadcast t·ªõi t·∫•t c·∫£ subscriber.
- Pure in-memory broadcast tr√™n m·ªói server: t·ªëc ƒë·ªô cao nh·∫•t (latency d∆∞·ªõi 10ms).

**L√Ω do 4: ƒê·ªô ph·ª©c t·∫°p tri·ªÉn khai th·∫•p**

- Load balancer IP Hash l√† feature chu·∫©n c√≥ trong m·ªçi LB (Nginx, HAProxy, AWS ALB).
- Kh√¥ng c·∫ßn thay ƒë·ªïi code ·ª©ng d·ª•ng, kh√¥ng c·∫ßn message broker infrastructure.
- Team c√≥ th·ªÉ tri·ªÉn khai trong 1-2 ng√†y, kh√¥ng c·∫ßn setup Redis cluster ho·∫∑c message queue ph·ª©c t·∫°p.

**L√Ω do 5: Chi ph√≠ infrastructure r·∫ª nh·∫•t**

- Ch·ªâ c·∫ßn 3-5 server WebSocket gateway + 1 load balancer (Nginx c√≥ th·ªÉ ch·∫°y container nh·∫π).
- Kh√¥ng c·∫ßn Redis cluster, kh√¥ng c·∫ßn message broker, kh√¥ng c·∫ßn additional storage.
- ∆Ø·ªõc t√≠nh chi ph√≠: 5 √ó $10 (server) + $5 (load balancer) = $55/th√°ng (r·∫ª nh·∫•t).

**Nh∆∞·ª£c ƒëi·ªÉm:**

- N·∫øu 1 server crash, 333 client m·∫•t k·∫øt n·ªëi ph·∫£i reconnect t·ªõi server kh√°c (do IP hash kh√°c, c√≥ th·ªÉ v√†o server 2 ho·∫∑c 3).
- Kh√¥ng balanced: n·∫øu c√≥ 1 client VIP k·∫øt n·ªëi l√¢u d√†i, IP c·ªßa n√≥ s·∫Ω "d√≠nh" tr√™n server ƒë√≥, chi·∫øm t√†i nguy√™n kh√¥ng c√≥ c√°ch t√°i c√¢n b·∫±ng.
- Kh√≥ scale ngang: n·∫øu th√™m server t·ª´ 3 l√™n 10, ph·∫£i re-hash t·∫•t c·∫£ IP, g√¢y disruption cho ~90% client.

![Sticky Session Architecture](diagrams_images/sticky_session.png)

---

### Gi·∫£i Ph√°p 2: Redis Pub/Sub + Stateless Servers

**C∆° ch·∫ø ho·∫°t ƒë·ªông:**
T·∫•t c·∫£ WebSocket gateway server l√† stateless - kh√¥ng l∆∞u tr·∫°ng th√°i g√¨. Khi server 1 nh·∫≠n price update t·ª´ Binance:

1. Server 1 kh√¥ng broadcast tr·ª±c ti·∫øp t·ªõi client c·ªßa m√¨nh
2. Thay v√†o ƒë√≥, server 1 publish message t·ªõi Redis channel "price-update"
3. T·∫•t c·∫£ 3 server (1, 2, 3) subscribe t·ªõi channel n√†y
4. Redis broadcast message ƒë·∫øn 3 server c√πng l√∫c
5. M·ªói server nh·∫≠n message, r·ªìi broadcast t·ªõi client c·ªßa n√≥

V√≠ d·ª• flow:

```
Binance API ‚Üí Server 1 ‚Üí Redis Pub/Sub ‚Üí Server 1, 2, 3 ‚Üí T·∫•t c·∫£ 1000 client
```

ƒêi·ªÉm quan tr·ªçng: Client kh√¥ng "d√≠nh" v√†o server n√†o. Client A c√≥ th·ªÉ k·∫øt n·ªëi t·ªõi server 1, disconnect, r·ªìi reconnect t·ªõi server 3 m√† kh√¥ng c·∫ßn sync state. T·∫•t c·∫£ server ƒë·ªÅu broadcast price update cho t·∫•t c·∫£ client.

**T·∫°i sao ph·∫£i d√πng c√°ch n√†y?**

**L√Ω do 1: Tr√°nh rate limit c·ªßa API**

- N·∫øu c√≥ 3 server, m·ªói server subscribe t·ªõi Binance API stream ri√™ng, s·∫Ω c√≥ 3 connection t·ªõi Binance.
- Binance c√≥ rate limit: ~40 k·∫øt n·ªëi/ph√∫t t·ª´ c√πng IP.
- D√πng Redis Pub/Sub: ch·ªâ c·∫ßn 1 connection t·ªõi Binance (t·ª´ data ingestion service), publish v√†o Redis. 3 gateway server subscribe t·ª´ Redis (kh√¥ng consume Binance rate limit).

**L√Ω do 2: ƒê·∫£m b·∫£o t√≠nh nh·∫•t qu√°n d·ªØ li·ªáu**

- N·∫øu m·ªói server subscribe t·ªõi Binance ri√™ng, c√≥ kh·∫£ nƒÉng:


---

## üìë QUICK NAVIGATION & INDEX

T√†i li·ªáu n√†y t·ªïng h·ª£p to√†n b·ªô n·ªôi dung b√°o c√°o ƒë·ªì √°n Web N√¢ng Cao ph√¢n t√≠ch ki·∫øn tr√∫c h·ªá th·ªëng ph√¢n t√°n.

### N·∫øu b·∫°n mu·ªën...

- **ƒê·ªçc to√†n b·ªô b√°o c√°o**: Ti·∫øp t·ª•c ƒë·ªçc t·ª´ d√≤ng 100 tr·ªü ƒëi
- **Hi·ªÉu WebSocket Scaling (Scenario 1)**: Xem ph·∫ßn "T√åNH HU·ªêNG 1" 
- **Hi·ªÉu Crawler Resilience (Scenario 2)**: Xem ph·∫ßn "T√åNH HU·ªêNG 2"
- **Hi·ªÉu Security Architecture (Scenario 3)**: Xem ph·∫ßn "T√åNH HU·ªêNG 3"
- **Xem s∆° ƒë·ªì ki·∫øn tr√∫c**: Xem Appendix (ph·∫ßn cu·ªëi) ho·∫∑c folder `diagrams_images/`
- **Tri·ªÉn khai h·ªá th·ªëng**: Xem ph·∫ßn Implementation Roadmap & Troubleshooting
- **Hi·ªÉu cost/ROI**: Xem ph·∫ßn Metrics & Cost Analysis

### üìä Diagrams C√≥ S·∫µn

- 13 PNG images trong `diagrams_images/` folder
- 7 detailed ASCII diagrams trong Appendix
- 5 PlantUML source files trong `diagrams/` folder

---

# B√°o C√°o ƒê·ªì √Ån Web N√¢ng Cao: Ph√¢n T√≠ch Ki·∫øn Tr√∫c H·ªá Th·ªëng Ph√¢n T√°n

## T√åNH HU·ªêNG 1: SCALING WEBSOCKET CHO 1000+ CLIENT K·∫æT N·ªêI ƒê·ªíNG TH·ªúI

### Ph√¢n T√≠ch B·∫£n Ch·∫•t V·∫•n ƒê·ªÅ

Trong ·ª©ng d·ª•ng ph√¢n t√≠ch gi√° ti·ªÅn m√£ h√≥a, ng∆∞·ªùi d√πng k·∫øt n·ªëi WebSocket ƒë·ªÉ nh·∫≠n c·∫≠p nh·∫≠t gi√° theo th·ªùi gian th·ª±c t·ª´ c√°c s√†n giao d·ªãch (Binance, Kraken, Coinbase). Khi s·ªë l∆∞·ª£ng kh√°ch h√†ng tƒÉng t·ª´ 100 l√™n 1000+ ng∆∞·ªùi k·∫øt n·ªëi ƒë·ªìng th·ªùi, m·ªôt server duy nh·∫•t kh√¥ng th·ªÉ x·ª≠ l√Ω ƒë∆∞·ª£c:

- **Gi·ªõi h·∫°n t√†i nguy√™n**: M·ªói WebSocket connection ti√™u t·ªën ~1-2MB b·ªô nh·ªõ. 1000 connections = ~1-2GB RAM cho c√°c socket objects.
- **V·∫•n ƒë·ªÅ broadcast**: Khi nh·∫≠n d·ªØ li·ªáu gi√° t·ª´ Binance, server ph·∫£i g·ª≠i l·∫°i ƒë·∫øn t·∫•t c·∫£ 1000 client. N·∫øu server x·ª≠ l√Ω tu·∫ßn t·ª±, s·∫Ω b·ªã ngh·∫Ωn.
- **Single Point of Failure**: N·∫øu server b·ªã crash, t·∫•t c·∫£ 1000 client m·∫•t k·∫øt n·ªëi v√† ph·∫£i reconnect t·ª´ ƒë·∫ßu, g√¢y "thundering herd" (b√£o request).
- **Kh√¥ng th·ªÉ scale horizontally**: N·∫øu th√™m server th·ª© 2, client c·ªßa server 1 kh√¥ng th·ªÉ communicate ƒë∆∞·ª£c v·ªõi client c·ªßa server 2 (ch√∫ng n·∫±m trong memory ri√™ng bi·ªát).

![WebSocket Problem Analysis](diagrams_images/websocket_problem.png)

### Gi·∫£i Ph√°p 1: Sticky Session v·ªõi IP Hash Load Balancer

**C∆° ch·∫ø ho·∫°t ƒë·ªông:**
S·ª≠ d·ª•ng Nginx ho·∫∑c HAProxy v·ªõi IP Hash strategy. Khi client A (IP: 203.0.113.5) k·∫øt n·ªëi l·∫ßn ƒë·∫ßu, load balancer hash IP n√†y ƒë·ªÉ map ƒë·∫øn server 1. M·ªçi request ti·∫øp theo t·ª´ c√πng IP s·∫Ω lu√¥n route t·ªõi server 1. V√¨ v·∫≠y WebSocket connection ƒë∆∞·ª£c "d√≠nh" v√†o server n√†y su·ªët v√≤ng ƒë·ªùi.

V·ªõi 3 server:

- Server 1: Client A, B, C, ... (~333 clients)
- Server 2: Client D, E, F, ... (~333 clients)
- Server 3: Client G, H, I, ... (~334 clients)

Khi nh·∫≠n d·ªØ li·ªáu gi√° t·ª´ Binance, m·ªói server independently broadcast t·ªõi nh√≥m client c·ªßa m√¨nh, gi·∫£m t·∫£i t·ª´ 1000 xu·ªëng ~333 tr√™n m·ªói server.

**T·∫°i sao ph·∫£i d√πng c√°ch n√†y?**

**L√Ω do 1: B·∫£o to√†n tr·∫°ng th√°i connection d·ªÖ d√†ng**

- WebSocket l√† stateful protocol: m·ªói connection c√≥ buffer ri√™ng, session context, authentication state. N·∫øu client k·∫øt n·ªëi t·ªõi server kh√°c, ph·∫£i thi·∫øt l·∫≠p l·∫°i t·ª´ ƒë·∫ßu.
- Sticky session ƒë∆°n gi·∫£n: kh√¥ng c·∫ßn shared state database, kh√¥ng c·∫ßn synchronize gi·ªØa c√°c server. M·ªói server qu·∫£n l√Ω ri√™ng nh√≥m client c·ªßa m√¨nh.
- IP Hash l√† deterministic: c√πng IP lu√¥n hash c√πng server, n√™n n·∫øu client reconnect (m·∫•t signal WiFi r·ªìi l·∫°i k·∫øt n·ªëi), s·∫Ω v·ªÅ server c≈© ngay l·∫≠p t·ª©c.

**L√Ω do 2: Tr√°nh "Thundering Herd" problem trong reconnection**

- N·∫øu d√πng round-robin (load balancer ph√¢n ph·ªëi ng·∫´u nhi√™n), khi c√≥ s·ª± c·ªë, t·∫•t c·∫£ 1000 client reconnect c√πng l√∫c c√≥ th·ªÉ map t·ªõi server ng·∫´u nhi√™n.
- Khi ƒë√≥, n·∫øu c√≥ 200 client reconnect t·ªõi server 1 (thay v√¨ 333 c≈©), server s·∫Ω b·ªã shock t·ª´ spike traffic.
- IP Hash gi·ªØ nguy√™n mapping: 333 client c·ªßa server 1 lu√¥n reconnect v·ªÅ server 1, n√™n traffic ·ªïn ƒë·ªãnh, kh√¥ng c√≥ spike.

**L√Ω do 3: Chi ph√≠ network overhead th·∫•p**

- Sticky session kh√¥ng c·∫ßn g·ª≠i message qua network gi·ªØa c√°c server (kh√¥ng c·∫ßn message broker).
- So v·ªõi Redis Pub/Sub solution, kh√¥ng ph·∫£i ƒë·∫©y m·ªói price update t·ªõi Redis broker, r·ªìi Redis broadcast t·ªõi t·∫•t c·∫£ subscriber.
- Pure in-memory broadcast tr√™n m·ªói server: t·ªëc ƒë·ªô cao nh·∫•t (latency d∆∞·ªõi 10ms).

**L√Ω do 4: ƒê·ªô ph·ª©c t·∫°p tri·ªÉn khai th·∫•p**

- Load balancer IP Hash l√† feature chu·∫©n c√≥ trong m·ªçi LB (Nginx, HAProxy, AWS ALB).
- Kh√¥ng c·∫ßn thay ƒë·ªïi code ·ª©ng d·ª•ng, kh√¥ng c·∫ßn message broker infrastructure.
- Team c√≥ th·ªÉ tri·ªÉn khai trong 1-2 ng√†y, kh√¥ng c·∫ßn setup Redis cluster ho·∫∑c message queue ph·ª©c t·∫°p.

**L√Ω do 5: Chi ph√≠ infrastructure r·∫ª nh·∫•t**

- Ch·ªâ c·∫ßn 3-5 server WebSocket gateway + 1 load balancer (Nginx c√≥ th·ªÉ ch·∫°y container nh·∫π).
- Kh√¥ng c·∫ßn Redis cluster, kh√¥ng c·∫ßn message broker, kh√¥ng c·∫ßn additional storage.
- ∆Ø·ªõc t√≠nh chi ph√≠: 5 √ó $10 (server) + $5 (load balancer) = $55/th√°ng (r·∫ª nh·∫•t).

**Nh∆∞·ª£c ƒëi·ªÉm:**

- N·∫øu 1 server crash, 333 client m·∫•t k·∫øt n·ªëi ph·∫£i reconnect t·ªõi server kh√°c (do IP hash kh√°c, c√≥ th·ªÉ v√†o server 2 ho·∫∑c 3).
- Kh√¥ng balanced: n·∫øu c√≥ 1 client VIP k·∫øt n·ªëi l√¢u d√†i, IP c·ªßa n√≥ s·∫Ω "d√≠nh" tr√™n server ƒë√≥, chi·∫øm t√†i nguy√™n kh√¥ng c√≥ c√°ch t√°i c√¢n b·∫±ng.
- Kh√≥ scale ngang: n·∫øu th√™m server t·ª´ 3 l√™n 10, ph·∫£i re-hash t·∫•t c·∫£ IP, g√¢y disruption cho ~90% client.

![Sticky Session Architecture](diagrams_images/sticky_session.png)

---

### Gi·∫£i Ph√°p 2: Redis Pub/Sub + Stateless Servers

**C∆° ch·∫ø ho·∫°t ƒë·ªông:**
T·∫•t c·∫£ WebSocket gateway server l√† stateless - kh√¥ng l∆∞u tr·∫°ng th√°i g√¨. Khi server 1 nh·∫≠n price update t·ª´ Binance:

1. Server 1 kh√¥ng broadcast tr·ª±c ti·∫øp t·ªõi client c·ªßa m√¨nh
2. Thay v√†o ƒë√≥, server 1 publish message t·ªõi Redis channel "price-update"
3. T·∫•t c·∫£ 3 server (1, 2, 3) subscribe t·ªõi channel n√†y
4. Redis broadcast message ƒë·∫øn 3 server c√πng l√∫c
5. M·ªói server nh·∫≠n message, r·ªìi broadcast t·ªõi client c·ªßa n√≥

V√≠ d·ª• flow:

```
Binance API ‚Üí Server 1 ‚Üí Redis Pub/Sub ‚Üí Server 1, 2, 3 ‚Üí T·∫•t c·∫£ 1000 client
```

ƒêi·ªÉm quan tr·ªçng: Client kh√¥ng "d√≠nh" v√†o server n√†o. Client A c√≥ th·ªÉ k·∫øt n·ªëi t·ªõi server 1, disconnect, r·ªìi reconnect t·ªõi server 3 m√† kh√¥ng c·∫ßn sync state. T·∫•t c·∫£ server ƒë·ªÅu broadcast price update cho t·∫•t c·∫£ client.

**T·∫°i sao ph·∫£i d√πng c√°ch n√†y?**

**L√Ω do 1: Tr√°nh rate limit c·ªßa API**

- N·∫øu c√≥ 3 server, m·ªói server subscribe t·ªõi Binance API stream ri√™ng, s·∫Ω c√≥ 3 connection t·ªõi Binance.
- Binance c√≥ rate limit: ~40 k·∫øt n·ªëi/ph√∫t t·ª´ c√πng IP.
- D√πng Redis Pub/Sub: ch·ªâ c·∫ßn 1 connection t·ªõi Binance (t·ª´ data ingestion service), publish v√†o Redis. 3 gateway server subscribe t·ª´ Redis (kh√¥ng consume Binance rate limit).

**L√Ω do 2: ƒê·∫£m b·∫£o t√≠nh nh·∫•t qu√°n d·ªØ li·ªáu**

- N·∫øu m·ªói server subscribe t·ªõi Binance ri√™ng, c√≥ kh·∫£ nƒÉng:
  - Server 1 nh·∫≠n price $50.00 t·∫°i timestamp T
  - Server 2 nh·∫≠n price $49.99 t·∫°i timestamp T+100ms (do network delay kh√°c nhau)
- Client k·∫øt n·ªëi server 1 th·∫•y $50.00, client k·∫øt n·ªëi server 2 th·∫•y $49.99 ‚Üí confuse.
- Redis Pub/Sub: Single source of truth. Data ingestion service publish 1 message t·ªõi Redis, t·∫•t c·∫£ server nh·∫≠n message gi·ªëng h·ªát ‚Üí t·∫•t c·∫£ client th·∫•y gi√° nh·∫•t qu√°n.

**L√Ω do 3: Ki·∫øn tr√∫c stateless, d·ªÖ scale ngang**

- V·ªõi sticky session: th√™m server = rehash t·∫•t c·∫£ IP, g√¢y disruption.
- V·ªõi Redis Pub/Sub: th√™m server m·ªõi, ch·ªâ c·∫ßn subscribe t·ªõi Redis channel. Kh√¥ng c·∫ßn rehash, kh√¥ng c·∫ßn move client state. C√≥ th·ªÉ scale t·ª´ 3 server l√™n 10 server trong 30 gi√¢y.
- Ph√π h·ª£p v·ªõi Kubernetes HPA (horizontal pod autoscaler): khi CPU spike, auto spawn pod m·ªõi, pod m·ªõi t·ª± ƒë·ªông subscribe Redis, nh·∫≠n price update, broadcast cho client ‚Üí seamless scaling.

**L√Ω do 4: T√°ch bi·ªát concern (Separation of Concerns)**

- Gateway server ch·ªâ handle WebSocket connectivity logic
- Data ingestion service ch·ªâ handle Binance API connection
- Message broker (Redis) ch·ªâ handle message distribution
- M·ªói component c√≥ responsibility ri√™ng, d·ªÖ debug, d·ªÖ test, d·ªÖ swap component (thay Redis b·∫±ng Kafka n·∫øu c·∫ßn).

**L√Ω do 5: High Availability cho Redis**

- Redis Pub/Sub c√≥ th·ªÉ ch·∫°y cluster mode v·ªõi replication: master + 2 slaves.
- N·∫øu master crash, slave t·ª± ƒë·ªông failover, ch·ªâ m·∫•t ~1-2 message (RTO ~100ms).
- Client reconnect seamlessly v√¨ gateway server stateless, kh√¥ng c·∫ßn recover state.

**Nh∆∞·ª£c ƒëi·ªÉm:**

- Latency cao h∆°n sticky session (ph·∫£i ƒëi qua Redis): 20-50ms thay v√¨ 10ms.
- Chi ph√≠ infrastructure cao h∆°n: c·∫ßn Redis cluster (~$50-100/th√°ng).
- Complexity cao h∆°n: ph·∫£i setup Redis, debug message loss, handle pub/sub failure.
- N·∫øu Redis broker b·ªã ƒë∆°, t·∫•t c·∫£ 1000 client kh√¥ng nh·∫≠n ƒë∆∞·ª£c price update.

![Redis Pub/Sub Architecture](diagrams_images/redis_pubsub.png)

---

### Gi·∫£i Ph√°p 3: Rendezvous Hashing (HRW - Highest Random Weight)

**C∆° ch·∫ø ho·∫°t ƒë·ªông:**
Thay v√¨ IP Hash (mapping IP ‚Üí server), d√πng Rendezvous Hashing: client t√≠nh to√°n ƒë√≠ch server d·ª±a tr√™n client ID + server list.

C√¥ng th·ª©c: V·ªõi client ID "user_123", server list [s1, s2, s3]:

```
score_s1 = hash("user_123" + "s1") = 0x5a2f...
score_s2 = hash("user_123" + "s2") = 0x3b1c...
score_s3 = hash("user_123" + "s3") = 0x7d9e...
‚Üí Choose s3 (highest score)
```

Client t·ª± quy·∫øt ƒë·ªãnh: "t√¥i s·∫Ω k·∫øt n·ªëi t·ªõi server 3". Kh√¥ng c·∫ßn load balancer IP Hash layer.

Khi add server 4:

```
score_s1 = 0x5a2f
score_s2 = 0x3b1c
score_s3 = 0x7d9e
score_s4 = 0x6f4a  (new)
‚Üí Choose s4
```

Ch·ªâ client k·∫øt n·ªëi t·ªõi s4 s·∫Ω remapping. Client c·ªßa s1, s2, s3 v·∫´n gi·ªØ nguy√™n mapping ‚Üí **minimal disruption** (~25% client remapping).

**T·∫°i sao ph·∫£i d√πng c√°ch n√†y?**

**L√Ω do 1: Scale ngang kh√¥ng disruption, kh√¥ng c·∫ßn Redis**

- Sticky session (IP Hash): th√™m server = rehash, ~90% client remapping, b√£o reconnect.
- Rendezvous Hashing: th√™m server = ch·ªâ ~25% client remapping, chia nh·∫π.
- Kh√¥ng c·∫ßn Redis broker (ti·∫øt ki·ªám chi ph√≠, gi·∫£m complexity), nh∆∞ng v·∫´n c√≥ benefit c·ªßa scaling.

**L√Ω do 2: Load balancer stateless, cost th·∫•p**

- Kh√¥ng c·∫ßn IP Hash logic ph·ª©c t·∫°p tr√™n load balancer.
- Client t·ª± calculate, client t·ª± connect. Load balancer ch·ªâ c·∫ßn forward connection ‚Üí DNS lookup ho·∫∑c service discovery.
- Th·∫≠m ch√≠ c√≥ th·ªÉ skip load balancer, d√πng DNS round-robin + client-side Rendezvous Hashing.

**L√Ω do 3: Latency th·∫•p, kh√¥ng qua Redis**

- Pure peer-to-peer: client ‚Üí server, kh√¥ng qua message broker.
- Latency: 10ms (gi·ªëng sticky session), kh√¥ng ph·∫£i 20-50ms (Redis route).
- Ph√π h·ª£p high-frequency trading apps c·∫ßn latency ultra-low.

**L√Ω do 4: Deterministic, predictable**

- Hash function deterministic: c√πng client ID, c√πng server list ‚Üí c√πng k·∫øt qu·∫£.
- N·∫øu client disconnect r·ªìi reconnect, c√πng IP s·∫Ω hash t·ªõi server c≈© (n·∫øu server list kh√¥ng ƒë·ªïi).
- D·ªÖ debug: trace client ID ‚Üí hash ‚Üí server, transparent.

**L√Ω do 5: Kh√¥ng ph·ª• thu·ªôc single broker**

- Sticky session: ph·ª• thu·ªôc load balancer health.
- Redis Pub/Sub: ph·ª• thu·ªôc Redis broker health (n·∫øu Redis crash = t·∫•t c·∫£ broadcast fail).
- Rendezvous Hashing: ph·ª• thu·ªôc server list gossip (peer-to-peer protocol). Ngay c·∫£ khi 1-2 server down, client v·∫´n t·ª± calculate v√† connect t·ªõi server c√≤n s·ªëng.

**Nh∆∞·ª£c ƒëi·ªÉm:**

- Require client-side logic: client c·∫ßn implement Rendezvous Hashing algorithm. Web client ph·∫£i include library (th√™m 20KB bundle size).
- Sticky session + broadcast: m·ªói server broadcast t·ªõi nh√≥m client c·ªßa m√¨nh, kh√¥ng nh∆∞ Redis sync.
- N·∫øu client list b·ªã skew (kh√¥ng ƒë·ªìng b·ªô), kh√≥ qu·∫£n l√Ω persistent connection.

![Rendezvous Hashing Architecture](diagrams_images/rendezvous_hashing.png)

---

### So S√°nh Ba Gi·∫£i Ph√°p

| Ti√™u Ch√≠                    | Sticky Session        | Redis Pub/Sub         | Rendezvous Hashing     |
| --------------------------- | --------------------- | --------------------- | ---------------------- |
| **Latency**                 | 10ms (th·∫•p nh·∫•t)      | 20-50ms               | 10ms                   |
| **Chi ph√≠ infrastructure**  | $55/th√°ng             | $100-150/th√°ng        | $60/th√°ng              |
| **ƒê·ªô ph·ª©c t·∫°p tri·ªÉn khai**  | ƒê∆°n (1-2 ng√†y)        | Trung b√¨nh (1 tu·∫ßn)   | Cao (2-3 tu·∫ßn)         |
| **Scaling (3‚Üí10 server)**   | 90% client disruption | Seamless, 30 gi√¢y     | 25% client disruption  |
| **Rate limit API**          | C√≥ (3 connection)     | Kh√¥ng (1 connection)  | C√≥ (3 connection)      |
| **Data consistency**        | Risky (3 source)      | Guaranteed (1 source) | Risky (3 source)       |
| **Single Point of Failure** | Load balancer         | Redis broker          | Gossip network         |
| **Monitoring/Debugging**    | D·ªÖ                    | Trung b√¨nh            | Kh√≥ (distributed hash) |

---

### Khuy·∫øn Ngh·ªã Cho Scenario 1

**Ng·∫Øn h·∫°n (0-3 th√°ng):** D√πng **Sticky Session + IP Hash**

- Ph√π h·ª£p v·ªõi timeline 2 th√°ng ph√°t tri·ªÉn ƒë·ªì √°n.
- Cost t·ªëi thi·ªÉu, complexity t·ªëi thi·ªÉu.
- Team c√≥ th·ªÉ t·∫≠p trung v√†o business logic, kh√¥ng ph·∫£i debug distributed system.
- Test kh·∫£ nƒÉng: 3 server √ó 330 clients/server = 1000 clients, latency < 50ms, reconnect time < 5 gi√¢y.

**Trung h·∫°n (3-6 th√°ng):** Migrate sang **Redis Pub/Sub**

- Khi c·∫ßn scale t·ªõi 5000+ clients, Sticky Session s·∫Ω b·ªã bottleneck (nh·∫•t qu√°n d·ªØ li·ªáu kh√¥ng ƒë·∫£m b·∫£o).
- Redis Pub/Sub cho ph√©p scale t·ªõi 10,000 clients v·ªõi stateless servers.
- Chu·∫©n b·ªã cho production deployment.

**D√†i h·∫°n (6+ th√°ng):** Consider **Rendezvous Hashing** n·∫øu latency l√† critical metric (high-frequency trading).

- N·∫øu revenue model ph·ª• thu·ªôc v√†o "ai nh·∫≠n gi√° tr∆∞·ªõc" (latency race), th√¨ Rendezvous Hashing worth the complexity.

---

## T√åNH HU·ªêNG 2: THU TH·∫¨P TIN T·ª®C KHI WEBSITE THAY ƒê·ªîI LI√äN T·ª§C

### Ph√¢n T√≠ch B·∫£n Ch·∫•t V·∫•n ƒê·ªÅ

H·ªá th·ªëng c·∫ßn thu th·∫≠p tin t·ª©c t·ª´ 50+ website (CoinTelegraph, The Block, crypto blogs, etc.) ƒë·ªÉ ph√¢n t√≠ch sentiment v√† t√°c ƒë·ªông t·ªõi gi√° crypto. M·ªói website th∆∞·ªùng xuy√™n thay ƒë·ªïi HTML structure:

- **CoinTelegraph** thay ƒë·ªïi CSS selector h√†ng tu·∫ßn: `.article-title` ‚Üí `.news-headline` ‚Üí `.post-header h1`
- **Medium** d√πng dynamic rendering: n·ªôi dung ƒë∆∞·ª£c load b·∫±ng JavaScript, kh√¥ng c√≥ trong static HTML
- **Forums/Reddit** c√≥ rate limiting nghi√™m kh·∫Øc: >100 request/ph√∫t = b·ªã block IP

**V·∫•n ƒë·ªÅ c·ª• th·ªÉ:**

- **Rule-based crawler (XPath/CSS selectors)** d·ª±a v√†o c·ªë ƒë·ªãnh selectors, n·∫øu website thay ƒë·ªïi ‚Üí data kh√¥ng extract ƒë∆∞·ª£c, n·∫øu update selector ‚Üí ph·∫£i redeploy code.
- **Single-threaded scraper** ch·∫°y tu·∫ßn t·ª±, 50 websites √ó 5 ph√∫t/website = 250 ph√∫t = g·∫ßn 4 gi·ªù, d·ªØ li·ªáu "stale".
- **Kh√¥ng th·ªÉ recovery**: n·∫øu crawler crash gi·ªØa website 25, ph·∫£i restart t·ª´ ƒë·∫ßu, m·∫•t 2 gi·ªù tr∆∞·ªõc khi v·ªÅ l·∫°i website 25.
- **API rate limit**: Website nghi v·∫•n crawler l√† bot ‚Üí block. C·∫ßn ph·∫£i rotate IP, user-agent, request header ƒë·ªÉ m√¥ ph·ªèng browser th·ª±c.

![News Crawler Problem](diagrams_images/crawler_problem.png)

### Gi·∫£i Ph√°p 1: Rule-Based Crawler v·ªõi XPath/CSS Selectors

**C∆° ch·∫ø ho·∫°t ƒë·ªông:**
D√πng BeautifulSoup ho·∫∑c lxml ƒë·ªÉ parse HTML, extract d·ªØ li·ªáu d·ª±a tr√™n XPath ho·∫∑c CSS selector c·ªë ƒë·ªãnh.

```
Website HTML ‚Üí Parse DOM ‚Üí XPath: //article[@class='news']/h1 ‚Üí Extract text ‚Üí L∆∞u DB
```

V√≠ d·ª•: ƒê·ªÉ extract ti√™u ƒë·ªÅ tin t·ª´ CoinTelegraph:

```
xpath: //h1[@class='post-title']
css:   h1.post-title
```

T·∫°o file config JSON ch·ª©a selector cho m·ªói website.

**T·∫°i sao ph·∫£i d√πng c√°ch n√†y?**

**L√Ω do 1: Hi·ªáu nƒÉng cao, latency th·∫•p**

- XPath/CSS selector match pattern tr·ª±c ti·∫øp, kh√¥ng c·∫ßn machine learning inference.
- Parse HTML + extract: 50-100ms/website (nhanh).
- So v·ªõi LLM: call API OpenAI 30-60 gi√¢y/website (t·ªën chi ph√≠, t·ªën th·ªùi gian).
- Ph√π h·ª£p real-time news aggregation: c·∫ßn tin m·ªõi trong v√≤ng 5 ph√∫t.

**L√Ω do 2: Chi ph√≠ th·∫•p nh·∫•t, kh√¥ng c·∫ßn LLM API quota**

- Rule-based: ch·ªâ c·∫ßn m√°y ch·ªß crawler + database, chi ph√≠ ~$50/th√°ng.
- LLM-based (g·ªçi OpenAI API): m·ªói extraction ~$0.02-0.05, 50 websites √ó 20 l·∫ßn/ng√†y = $20-50/ng√†y = $600-1500/th√°ng.
- Team c√≥ budget h·∫°n ch·∫ø (startup phase) ‚Üí Rule-based l√† must-have.

**L√Ω do 3: Ki·∫øm so√°t ho√†n to√†n, d·ªÖ debug**

- D·ªÖ inspect: xem tr·ª±c ti·∫øp XPath/CSS selector trong browser DevTools, bi·∫øt ngay t·∫°i sao extract fail.
- Kh√¥ng ph·∫£i ph√¢n t√≠ch "v√¨ sao AI model kh√¥ng extract" (black box problem v·ªõi LLM).
- Team c√≥ th·ªÉ update selector trong v√≤ng 5 ph√∫t m√† kh√¥ng c·∫ßn deploy code.

**L√Ω do 4: Kh√¥ng ph·ª• thu·ªôc external API, ƒë·ªô tin c·∫≠y cao**

- Rule-based ch·ªâ ph·ª• thu·ªôc website HTML structure (relative stable).
- LLM-based ph·ª• thu·ªôc OpenAI/Anthropic API: n·∫øu API down 1 gi·ªù, to√†n b·ªô crawler fail.
- C√≥ th·ªÉ offline mode: cache selector, ti·∫øp t·ª•c extract khi c√≥ HTML.

**L√Ω do 5: D·ªÖ parallelize, t·ªëi ∆∞u hi·ªáu nƒÉng**

- Selector matching l√† stateless operation, c√≥ th·ªÉ parallelize d·ªÖ d√†ng.
- Ch·∫°y 10 worker threads: 50 websites √∑ 10 = 5 websites/thread, xong trong 25 ph√∫t (gi·∫£m t·ª´ 250 ph√∫t).
- Kh√¥ng c·∫ßn queue ph·ª©c t·∫°p, kh√¥ng c·∫ßn LLM rate limit management.

**Nh∆∞·ª£c ƒëi·ªÉm:**

- **Fragile**: b·∫•t c·ª© thay ƒë·ªïi HTML ‚Üí selector fail ‚Üí no data extraction.
- **Manual maintenance**: m·ªói website thay ƒë·ªïi selector ‚Üí ph·∫£i update config (labor intensive).
- **Limited adaptability**: n·∫øu website t√≠nh ti·ªÅn cho API (e.g., Bloomberg) ‚Üí kh√¥ng th·ªÉ extract.
- **Anti-bot detection**: n·∫øu website detect pattern extraction ‚Üí block IP.

![Rule-Based Crawler](diagrams_images/rule_based_crawler.png)

---

### Gi·∫£i Ph√°p 2: LLM-Based Parser (AI-Powered Content Extraction)

**C∆° ch·∫ø ho·∫°t ƒë·ªông:**
G·ª≠i raw HTML + extraction instruction t·ªõi LLM (OpenAI GPT-4, Claude 3):

```
Input: HTML raw + prompt "Extract news title, author, publish date, summary"
Output: {"title": "...", "author": "...", "date": "...", "summary": "..."}
```

LLM hi·ªÉu semantic: d√π website thay ƒë·ªïi CSS class t·ª´ `.article-title` th√†nh `.headline`, LLM v·∫´n recognize "ƒë√¢y l√† ti√™u ƒë·ªÅ" d·ª±a v√†o v·ªã tr√≠, font size, context.

**T·∫°i sao ph·∫£i d√πng c√°ch n√†y?**

**L√Ω do 1: Robust t·ªõi website structure changes**

- Rule-based fail khi HTML thay ƒë·ªïi. LLM fail hi·∫øm h∆°n (semantic understanding).
- V√≠ d·ª•: Website thay ƒë·ªïi HTML t·ª´ `<h1 class="title">` th√†nh `<p class="headline">`, rule-based fail ngay, LLM v·∫´n extract.
- Gi·∫£m manual maintenance: kh√¥ng ph·∫£i update selector h√†ng tu·∫ßn.

**L√Ω do 2: Extract metadata ph·ª©c t·∫°p**

- Rule-based kh√≥ extract: "author name n·∫±m ch·ªó n√†o?" (c√≥ th·ªÉ `.by-author`, `.article-meta span:nth-child(2)`, etc.).
- LLM d·ªÖ d√†ng: "extract author, publish date, article sentiment" t·ª´ raw text, d√πng semantic reasoning.
- Ph√π h·ª£p: news sentiment analysis, extract price mentions, financial impact keywords.

**L√Ω do 3: Adaptive parsing cho dynamic/JavaScript-rendered content**

- Rule-based ch·ªâ parse static HTML, n·∫øu content load qua JavaScript (AJAX) ‚Üí kh√¥ng th·∫•y.
- LLM + headless browser (Playwright): render JavaScript, r·ªìi pass rendered HTML t·ªõi LLM.
- Ph√π h·ª£p modern websites (React, Vue SPA).

**L√Ω do 4: Multi-language support out-of-box**

- Rule-based: CSS selector `.article-title` gi·ªëng nhau, nh∆∞ng ti√™u ƒë·ªÅ c√≥ th·ªÉ ti·∫øng Anh, Trung, Vi·ªát.
- LLM multi-lingual: "extract title" ‚Üí hi·ªÉu t·∫•t c·∫£ ng√¥n ng·ªØ, t·ª± translate t·ªõi English n·∫øu c·∫ßn.
- Kh√¥ng c·∫ßn maintain parser ri√™ng cho m·ªói language.

**L√Ω do 5: Content quality filtering built-in**

- Rule-based extract to√†n b·ªô `.article-title`, k·ªÉ c·∫£ ads, fake news, spam.
- LLM c√≥ th·ªÉ evaluate: "Is this legitimate news or spam/advertisement?" ‚Üí filter quality.
- Gi·∫£m noise data, improve downstream analytics.

**Nh∆∞·ª£c ƒëi·ªÉm:**

- **Chi ph√≠ cao**: OpenAI API ~$0.02-0.05/request, 50 websites √ó 20 l·∫ßn/ng√†y = $20-50/ng√†y.
- **Latency cao**: 30-60 gi√¢y/website (LLM inference + API latency).
- **Rate limit**: OpenAI/Anthropic c√≥ TPM (tokens per minute) limit. Ph·∫£i implement retry logic, queue management.
- **Dependency external API**: n·∫øu OpenAI down ‚Üí crawler fail ho√†n to√†n.
- **Hallucination risk**: LLM c√≥ th·ªÉ generate fake data (e.g., made-up author name).

![LLM-Based Crawler](diagrams_images/llm_crawler.png)

---

### Gi·∫£i Ph√°p 3: Hybrid Crawler (Rule-Based + LLM Fallback)

**C∆° ch·∫ø ho·∫°t ƒë·ªông:**
Ch·∫°y rule-based crawler tr∆∞·ªõc (fast, cheap). N·∫øu extraction confidence < 70% ho·∫∑c selector not found ‚Üí fallback t·ªõi LLM.

```
1. Try XPath selector ‚Üí found & confidence=95% ‚Üí return result
2. Try CSS selector ‚Üí not found ‚Üí fallback to LLM
3. LLM parse + extract ‚Üí return result
4. If LLM fail ‚Üí mark as failed, log error
```

C·ª• th·ªÉ flow:

- Website A (stable): rule-based extract th√†nh c√¥ng 98% times ‚Üí rarely call LLM
- Website B (frequently change): rule-based extract 40% times ‚Üí fallback LLM 60% times
- Website C (dynamic content): rule-based extract 0% times ‚Üí always use LLM

**T·∫°i sao ph·∫£i d√πng c√°ch n√†y?**

**L√Ω do 1: Optimized cost + robustness trade-off**

- Rule-based cho 70% websites (stable HTML): 0 LLM cost.
- LLM cho 30% websites (unstable/dynamic): ~$5-10/ng√†y.
- Gi·∫£m cost t·ª´ $600/th√°ng (pure LLM) xu·ªëng $200-300/th√°ng (hybrid).
- Gi·ªØ robustness: 30% websites v·∫´n extract ƒë∆∞·ª£c khi HTML thay ƒë·ªïi.

**L√Ω do 2: Latency optimization: fast path + fallback**

- Rule-based fast path: 50-100ms (nhanh).
- LLM slow path: 30-60 gi√¢y (khi c·∫ßn).
- Avg latency: 70% √ó 100ms + 30% √ó 30s = 70ms + 9s = 9.07s (h·ª£p l√Ω cho batch job).
- Pure LLM avg latency: 30-60s (qu√° ch·∫≠m cho real-time aggregation).

**L√Ω do 3: Graceful degradation under failures**

- Rule-based fail: fallback LLM (99% success rate overall).
- LLM fail: fallback text similarity matching (fuzzy search).
- N·∫øu to√†n b·ªô fail ‚Üí mark item as "review needed" ‚Üí manual review sau.
- Tr√°nh "no result" scenario: lu√¥n c√≥ fallback plan.

**L√Ω do 4: Incremental improvement pipeline**

- Collect extraction failure cases (rule-based fail, LLM fail).
- Analyze failure pattern: "Website X selector thay ƒë·ªïi t·∫°i th·ªùi ƒëi·ªÉm Y".
- Update selector ho·∫∑c add LLM instruction refinement.
- A/B test: measure success rate improvement qua version.
- Data-driven optimization loop, kh√¥ng "guess and check".

**L√Ω do 5: Easy to monitor v√† debug**

- Metric: rule-based success rate, LLM success rate, fallback frequency.
- Alert: "Website X rule-based success rate drop t·ª´ 98% xu·ªëng 30%" ‚Üí investigate HTML change.
- Debug: xem trace log "which path (rule-based vs LLM) was used for extraction".
- Production observability: clear visibility t·ªõi system behavior.

**Nh∆∞·ª£c ƒëi·ªÉm:**

- **Complexity cao**: ph·∫£i maintain 2 extraction engines (rule-based + LLM).
- **Debugging harder**: khi extraction fail, kh√¥ng r√µ rule-based hay LLM responsible.
- **Inconsistent output format**: rule-based return `{title, author}`, LLM return `{title, author, sentiment}` ‚Üí need normalization.

![Hybrid Crawler](diagrams_images/hybrid_crawler.png)

---

### So S√°nh Ba Gi·∫£i Ph√°p Crawler

| Ti√™u Ch√≠                     | Rule-Based            | LLM-Based             | Hybrid                            |
| ---------------------------- | --------------------- | --------------------- | --------------------------------- |
| **Latency/website**          | 50-100ms              | 30-60s                | 100ms (fast path), 30s (fallback) |
| **Chi ph√≠/th√°ng**            | ~$50                  | ~$600-1500            | ~$200-300                         |
| **Success rate HTML change** | 20-40%                | 85-95%                | 90-98%                            |
| **Complexity tri·ªÉn khai**    | ƒê∆°n (1 tu·∫ßn)          | Trung b√¨nh (2-3 tu·∫ßn) | Cao (3-4 tu·∫ßn)                    |
| **Maintenance effort**       | Cao (update selector) | Th·∫•p (update prompt)  | Trung b√¨nh                        |
| **Scalability (100+ sites)** | Kh√≥                   | D·ªÖ (LLM scale)        | D·ªÖ                                |
| **External dependency**      | Th·∫•p                  | Cao (OpenAI API)      | Trung b√¨nh                        |
| **Production readiness**     | Ph√π h·ª£p MVP           | Ph√π h·ª£p mature        | Ph√π h·ª£p production                |

---

### Khuy·∫øn Ngh·ªã Cho Scenario 2

**Ng·∫Øn h·∫°n (0-1 th√°ng, MVP):** D√πng **Rule-Based Crawler**

- Ch·ªçn 10 website stable: CoinTelegraph, The Block, Crypto Reddit, Twitter Finance tags.
- Setup XPath selector config cho m·ªói website.
- Extract + store t·ªõi database, display real-time feed.
- Demo t√≠nh nƒÉng news aggregation cho user.
- Cost: $50/th√°ng, Team effort: 1 tu·∫ßn.

**Trung h·∫°n (1-2 th√°ng, polish):** Expand + Hybrid approach

- Th√™m 40 websites n·ªØa (m·ª•c ti√™u 50+ sites).
- Implement fallback logic: rule-based ‚Üí LLM n·∫øu fail.
- Monitor metric: success rate, latency.
- Refine selector config d·ª±a tr√™n failure log.
- Cost: $200-300/th√°ng, Team effort: 2-3 tu·∫ßn.

**D√†i h·∫°n (2+ th√°ng, optimize):** Pure LLM ho·∫∑c advanced hybrid

- N·∫øu team mu·ªën semantic analysis (sentiment, entities, relationships) ‚Üí pure LLM advantage.
- N·∫øu team mu·ªën cost optimization ‚Üí hybrid is sweet spot.
- Consider multi-modal: image extraction t·ª´ charts, infographics.

---

## T√åNH HU·ªêNG 3: B·∫¢O M·∫¨T TRONG H·ªÜ TH·ªêNG PH√ÇN T√ÅN

### Ph√¢n T√≠ch B·∫£n Ch·∫•t V·∫•n ƒê·ªÅ

H·ªá th·ªëng ph√¢n t√≠ch gi√° crypto x·ª≠ l√Ω d·ªØ li·ªáu nh·∫°y c·∫£m: portfolio ng∆∞·ªùi d√πng, l·ªánh giao d·ªãch, keys API c·ªßa exchanges. Khi h·ªá th·ªëng scale ngang t·ªõi 10+ microservices (API Gateway, Auth Service, WebSocket Gateway, Price Service, News Service, etc.), c√°c v·∫•n ƒë·ªÅ b·∫£o m·∫≠t ph√°t sinh:

- **T·∫•n c√¥ng horizontal**: Hacker hack ƒë∆∞·ª£c 1 microservice (e.g., News Service) ‚Üí c√≥ quy·ªÅn truy c·∫≠p c√°c service kh√°c m√† kh√¥ng ph·∫£i authenticate l·∫°i.
- **Token theft**: Client token b·ªã steal (XSS, man-in-the-middle) ‚Üí hacker c√≥ th·ªÉ gi·∫£ danh user, truy c·∫≠p portfolio, thay ƒë·ªïi settings.
- **API key leak**: Developer commit AWS keys, OpenAI API key v√†o git ‚Üí public repo ‚Üí hacker d√πng keys ƒë·ªÉ spam API calls.
- **DDoS attack**: Hacker g·ª≠i request l·ªõn t·ªõi API Gateway, l√†m crash service ‚Üí user kh√¥ng th·ªÉ access.
- **Data breach**: N·∫øu database b·ªã compromise ‚Üí t·∫•t c·∫£ user data (email, portfolio, encrypted keys) b·ªã dump.
- **Audit trail missing**: Khi c√≥ s·ª± c·ªë, kh√¥ng bi·∫øt ai ƒë√£ truy c·∫≠p g√¨, khi n√†o ‚Üí kh√≥ compliance, kh√≥ debug security incident.

![Security Threats](diagrams_images/security_threats.png)

### Gi·∫£i Ph√°p 1: JWT (JSON Web Tokens) + Token Rotation

**C∆° ch·∫ø ho·∫°t ƒë·ªông:**
Auth Service t·∫°o JWT token (√≠t quy·ªÅn, short-lived, 15 ph√∫t expiry) + Refresh Token (quy·ªÅn cao, long-lived, 7 ng√†y). Client l∆∞u c·∫£ 2 token.

Flow:

```
1. User login ‚Üí Auth Service verify password ‚Üí return {access_token, refresh_token}
2. Client call API ‚Üí include access_token trong header Authorization: Bearer <access_token>
3. API Gateway verify token signature (kh√¥ng g·ªçi Auth Service)
4. N·∫øu access_token expire ‚Üí client d√πng refresh_token ƒë·ªÉ get token m·ªõi
5. Refresh_token c≈©ng c√≥ th·ªÉ rotate: nh√¢n m·ªói refresh, c·∫•p refresh_token m·ªõi
```

Ch√¨a kh√≥a: Access token c√≥ short lifetime (15 min) ‚Üí k·ªÉ c·∫£ b·ªã steal c≈©ng ch·ªâ valid 15 ph√∫t. Sau ƒë√≥ hacker c·∫ßn refresh_token ƒë·ªÉ extend, n·∫øu refresh_token c≈©ng rotate ‚Üí harder to exploit.

**T·∫°i sao ph·∫£i d√πng c√°ch n√†y?**

**L√Ω do 1: Stateless authentication, scalable**

- Traditional session (server l∆∞u session dict): n·∫øu 3 server, m·ªói server ph·∫£i sync session ‚Üí complex.
- JWT: m·ªói server c√≥ th·ªÉ verify token signature independently (public key, kh√¥ng c·∫ßn call central Auth Service).
- Scale t·ªõi 100 server: v·∫´n kh√¥ng c·∫ßn centralized session store, kh√¥ng c·∫ßn bottleneck.

**L√Ω do 2: Short-lived access token gi·∫£m risk**

- N·∫øu access token b·ªã steal: hacker only c√≥ 15 ph√∫t ƒë·ªÉ exploit tr∆∞·ªõc token expire.
- So v·ªõi traditional session (usually 1-2 gi·ªù validity): 15 ph√∫t = ~4-8x safer.
- Attacker ph·∫£i work fast ho·∫∑c have refresh_token ƒë·ªÉ extend, complexity cao h∆°n.

**L√Ω do 3: Token rotation reduce breach impact**

- M·ªói refresh ‚Üí c·∫•p refresh_token m·ªõi ‚Üí old refresh_token invalid.
- N·∫øu hacker c√≥ stale refresh_token, kh√¥ng th·ªÉ d√πng ‚Üí automatic revocation.
- D√π database b·ªã leak, old tokens c≈©ng v√¥ d·ª•ng (d√π b·ªã decrypt).

**L√Ω do 4: Flexible permission model (JWT claims)**

- Token ch·ª©a claims: user_id, role, permissions, scope.
- Kh√¥ng c·∫ßn g·ªçi DB m·ªói request ‚Üí latency low (m·ªói request kh√¥ng ph·∫£i query DB).
- Microservice c√≥ th·ªÉ verify scope tr·ª±c ti·∫øp: "token c√≥ permission 'transfer'?" ‚Üí check claim.

**L√Ω do 5: Revocation support (blacklist)**

- C√≥ th·ªÉ implement token blacklist (Redis): logout ‚Üí th√™m token v√†o blacklist.
- Revocation query Redis (~5ms) ‚Üí fast.
- So v·ªõi traditional session: session ph·∫£i qu·∫£n l√Ω lifecycle (creation, update, deletion).

**Nh∆∞·ª£c ƒëi·ªÉm:**

- **Token revocation delay**: token v·∫´n valid t·ªõi expiry, blacklist check l√† fallback.
- **Token size**: JWT c√≥ th·ªÉ l·ªõn n·∫øu claims nhi·ªÅu, m·ªói request ph·∫£i send JWT ‚Üí overhead.
- **Refresh token management**: ph·∫£i carefully handle rotation, prevent old token reuse (need additional logic).

![JWT Architecture](diagrams_images/jwt_architecture.png)

---

### Gi·∫£i Ph√°p 2: Zero-Trust Architecture + Service-to-Service Authentication

**C∆° ch·∫ø ho·∫°t ƒë·ªông:**
Gi·∫£ ƒë·ªãnh kh√¥ng c√≥ "trusted internal network". M·ªói request gi·ªØa service ph·∫£i authenticate, kh√¥ng implicit trust.

Flow:

```
API Gateway ‚Üí Price Service:
  1. API Gateway g·ª≠i request + S2S JWT (service-to-service token)
  2. Price Service verify S2S JWT signature (d√πng public key c·ªßa API Gateway)
  3. N·∫øu valid ‚Üí process request. N·∫øu invalid ‚Üí reject
```

T∆∞∆°ng t·ª± cho m·ªçi service-to-service call (Price Service ‚Üí News Service, News Service ‚Üí Analytics Service, etc.).

M·ªói service c√≥ certificate (mTLS) + S2S JWT ‚Üí double authentication.

**T·∫°i sao ph·∫£i d√πng c√°ch n√†y?**

**L√Ω do 1: Isolate compromise impact**

- Hacker hack News Service ‚Üí c√≥ access S2S token c·ªßa News Service, nh∆∞ng kh√¥ng c√≥ token c·ªßa Price Service.
- N·∫øu hack Price Service, c·∫ßn ri√™ng token c·ªßa Price Service.
- D√π hack ƒë∆∞·ª£c 1 service, kh√¥ng t·ª± ƒë·ªông c√≥ access t·∫•t c·∫£ service ‚Üí compartmentalize breach.

**L√Ω do 2: Audit trail service-to-service**

- M·ªói service-to-service call log: who called, when, what action.
- N·∫øu c√≥ suspicious activity (News Service suddenly call Analytics Service 1000 times) ‚Üí detect anomaly.
- Breach investigation: trace which service made unauthorized call ‚Üí pinpoint attacker entry point.

**L√Ω duo 3: Defense in depth**

- User request ‚Üí API Gateway (verify user JWT) ‚Üí Price Service (verify S2S JWT) ‚Üí check RBAC roles.
- Multiple layers: user identity layer, service identity layer, authorization layer.
- Even n·∫øu 1 layer bypass, c√≤n 2 layers ƒë·ªÉ stop attacker.

**L√Ω do 4: Encryption in transit (mTLS)**

- T·∫•t c·∫£ service-to-service communication d√πng mTLS (mutual TLS).
- Network sniffer kh√¥ng th·ªÉ ƒë·ªçc traffic (encrypted).
- Service ph·∫£i present certificate ‚Üí server verify client identity ‚Üí bidirectional auth.

**L√Ω do 5: Revocation at scale**

- Service revoke certificate c·ªßa compromised service ‚Üí automatic revocation cho t·∫•t c·∫£ connection.
- Kh√¥ng ph·∫£i restart service, kh√¥ng ph·∫£i deploy code, ch·ªâ update certificate store.
- Revocation immediate n·∫øu d√πng OCSP (Online Certificate Status Protocol).

**Nh∆∞·ª£c ƒëi·ªÉm:**

- **Operational complexity**: ph·∫£i manage certificates cho 10+ services, rotation schedule, OCSP setup.
- **Performance overhead**: mTLS handshake th√™m latency (~50-100ms per service call).
- **Debugging harder**: khi service call fail, maybe certificate expire, maybe network issue, hard to trace.

![Zero-Trust Architecture](diagrams_images/zero_trust.png)

---

### Gi·∫£i Ph√°p 3: Multi-Layer Security + API Gateway + Rate Limiting + Audit Logging

**C∆° ch·∫ø ho·∫°t ƒë·ªông:**
K·∫øt h·ª£p nhi·ªÅu layer:

```
User Request ‚Üí CloudFlare (DDoS mitigation) ‚Üí Nginx (rate limiting) ‚Üí API Gateway
(auth check) ‚Üí Microservice (RBAC check) ‚Üí Database (audit log)
```

C·ª• th·ªÉ:

1. **CloudFlare**: Gi·∫£m DDoS request, block suspicious IP, cache static content.
2. **Nginx**: Rate limit: max 1000 req/ph√∫t/IP. N·∫øu v∆∞·ª£t ‚Üí 429 Too Many Requests.
3. **API Gateway**: Verify user JWT token, check expiry, extract user_id + permissions.
4. **Microservice**: Check RBAC: "user c√≥ role 'admin'?" ‚Üí allow. Otherwise deny.
5. **Database**: Log m·ªói action: user_id, action (read/write/delete), timestamp, resource_id, IP.

**T·∫°i sao ph·∫£i d√πng c√°ch n√†y?**

**L√Ω do 1: Defense layers ngƒÉn ch·∫∑n different attack vectors**

- DDoS attack ‚Üí CloudFlare block.
- Brute force auth ‚Üí Nginx rate limit.
- Invalid token ‚Üí API Gateway reject.
- Unauthorized access (user kh√¥ng c√≥ permission) ‚Üí Microservice reject.
- Suspicious activity (access resource ƒë√™m n·ª≠a ƒë√™m) ‚Üí audit log flag.
- No single point of failure: even bypass 1 layer, still protected by others.

**L√Ω do 2: Rate limiting prevent API abuse**

- N·∫øu API key leak ‚Üí attacker g·ª≠i request spam (e.g., 10,000 req/ph√∫t).
- Rate limit 1000 req/ph√∫t ‚Üí attacker only get 1000, kh√¥ng th·ªÉ exhaust resource.
- Protect backend from cascade failure: n·∫øu 1 attacker spam, kh√¥ng crash service.

**L√Ω do 3: Audit logging enable post-breach analysis**

- Breach x·∫£y ra ‚Üí check audit log: "who accessed portfolio X, khi n√†o, IP n√†o".
- Trace attacker journey: first access IP, progression of actions, final access.
- Compliance requirement: ng√¢n h√†ng/t√†i ch√≠nh c·∫ßn audit trail cho regulatory.
- Root cause analysis: khi bug g√¢y data loss, check log ƒë·ªÉ understand sequence of events.

**L√Ω do 4: RBAC (Role-Based Access Control) principle of least privilege**

- User trader ch·ªâ c√≥ role 'trader' ‚Üí ch·ªâ read price, submit order, kh√¥ng delete user.
- User admin c√≥ role 'admin' ‚Üí full access.
- M·ªói role = set permissions. User n√†o c√≥ role n·∫•y.
- D√π token leak, attacker ch·ªâ c√≥ permission c·ªßa compromised user role (limited damage).

**L√Ω do 5: API Gateway centralize policy enforcement**

- Thay v√¨ implement auth logic ·ªü m·ªói microservice (Code duplication, inconsistent) ‚Üí implement once at gateway.
- Update auth policy ‚Üí update 1 place (API Gateway) ‚Üí all services inherit.
- Consistency: m·ªói microservice follow same auth rule, kh√¥ng c√≥ bypass.

**Nh∆∞·ª£c ƒëi·ªÉm:**

- **Latency overhead**: request ph·∫£i qua multiple layer (CloudFlare ‚Üí Nginx ‚Üí API Gateway ‚Üí Service) ‚Üí cumulative latency 100-300ms.
- **Operational complexity**: ph·∫£i configure CloudFlare, Nginx, API Gateway, RBAC rules, audit logging ‚Üí overhead.
- **Log storage cost**: audit logging produce huge volume (1M request/day √ó 50 fields ~ 50GB/month uncompressed) ‚Üí need compression, archival strategy.

![Multi-Layer Security](diagrams_images/multi_layer_security.png)

---

### So S√°nh Ba Gi·∫£i Ph√°p B·∫£o M·∫≠t

| Ti√™u Ch√≠                        | JWT + Token Rotation    | Zero-Trust + mTLS        | Multi-Layer + RBAC          |
| ------------------------------- | ----------------------- | ------------------------ | --------------------------- |
| **User auth robustness**        | Cao (short-lived token) | N/A (service-to-service) | Cao + RBAC                  |
| **Service-to-service security** | Kh√¥ng (implicit trust)  | Cao (certificate-based)  | Cao (JWT + certificate)     |
| **DDoS protection**             | Kh√¥ng                   | Kh√¥ng                    | Cao (rate limit)            |
| **Audit trail**                 | Partial (token level)   | Partial                  | Cao (comprehensive logging) |
| **Compliance ready**            | Partial (PCI DSS)       | Partial                  | Cao (PCI DSS + SOX ready)   |
| **Implementation complexity**   | ƒê∆°n (2-3 tu·∫ßn)          | Trung b√¨nh (3-4 tu·∫ßn)    | Cao (4-6 tu·∫ßn)              |
| **Operational cost**            | Th·∫•p ($50/th√°ng)        | Trung b√¨nh ($100/th√°ng)  | Cao ($200+/th√°ng)           |
| **Performance impact**          | Th·∫•p (~5ms)             | Trung b√¨nh (~50-100ms)   | Trung b√¨nh (~100-200ms)     |
| **Breach containment**          | Partial                 | Cao                      | Cao                         |

---

### Khuy·∫øn Ngh·ªã Cho Scenario 3

**Ng·∫Øn h·∫°n (0-1 th√°ng, MVP security):** D√πng **JWT + Token Rotation**

- Tri·ªÉn khai user authentication: login ‚Üí JWT access token (15 min) + refresh token (7 ng√†y).
- Implement token rotation: m·ªói refresh ‚Üí c·∫•p token m·ªõi.
- API Gateway verify token.
- Basic rate limiting: 1000 req/ph√∫t/user.
- Cost: ~$50/th√°ng, Team effort: 1 tu·∫ßn.

**Trung h·∫°n (1-2 th√°ng, enhance security):** Add **Zero-Trust + Service-to-Service Auth**

- Implement mTLS cho service-to-service communication.
- Each service get certificate, verify peer certificate.
- S2S JWT: m·ªói service t·∫°o JWT khi call service kh√°c.
- Audit service-to-service call.
- Cost: ~$100/th√°ng (cert management), Team effort: 2-3 tu·∫ßn.

**D√†i h·∫°n (2+ th√°ng, production-grade):** Full **Multi-Layer + RBAC + Comprehensive Audit**

- CloudFlare DDoS protection.
- Nginx advanced rate limiting (per user, per endpoint).
- API Gateway + RBAC engine: role-based access control.
- Comprehensive audit logging: m·ªói action log t·ªõi audit database.
- Alerts: suspicious pattern detection (brute force, data exfiltration).
- Cost: ~$200-300/th√°ng, Team effort: 4-6 tu·∫ßn.

---

## KI·∫æN TR√öC T·ªîNG H·ª¢P V√Ä B√ÄI H·ªåC R√öT RA

### T·ªïng Quan H·ªá Th·ªëng

H·ªá th·ªëng ph√¢n t√≠ch gi√° crypto k·∫øt h·ª£p 3 scenario:

1. **WebSocket Scaling**: S·ª≠ d·ª•ng Sticky Session (ng·∫Øn h·∫°n) + Redis Pub/Sub (trung h·∫°n) ƒë·ªÉ support 1000+ clients real-time price streaming.
2. **Crawler Resilience**: S·ª≠ d·ª•ng Hybrid approach (Rule-based + LLM fallback) ƒë·ªÉ robust against website HTML changes, gi·∫£m cost so v·ªõi pure LLM.
3. **Security Architecture**: Multi-layer defense (JWT + Zero-Trust + RBAC + audit logging) ƒë·ªÉ protect user data v√† prevent unauthorized access.

**Production deployment:**

- Frontend (Web + Mobile) ‚Üí CloudFlare (DDoS) ‚Üí Nginx (load balancer, rate limit) ‚Üí API Gateway (JWT verify, RBAC check) ‚Üí Microservices:
  - WebSocket Gateway (sticky session, broadcast)
  - Price Service (Redis Pub/Sub consumer)
  - Crawler Service (hybrid extraction)
  - Analytics Service (compute metrics)
  - Auth Service (JWT issue, refresh)
- Database: PostgreSQL + Redis cache
- Monitoring: Prometheus + Grafana (metrics), ELK (logs), Jaeger (tracing)

![Production Architecture](diagrams_images/production_architecture.png)

---

### B√†i H·ªçc R√∫t Ra

**1. Trade-off Analysis l√† core c·ªßa Architecture Design**

- Kh√¥ng c√≥ gi·∫£i ph√°p "ho√†n h·∫£o" (perfect): m·ªói solution ƒë·ªÅu c√≥ trade-off (cost vs latency, simplicity vs robustness, etc.).
- Engineer ph·∫£i hi·ªÉu trade-off r√µ r√†ng, r·ªìi choose d·ª±a v√†o business priority.
- V√≠ d·ª•: Rule-based crawler cheap nh∆∞ng brittle. LLM-based robust nh∆∞ng expensive. Hybrid balance c·∫£ hai.

**2. Incremental approach: MVP ‚Üí Polish ‚Üí Production**

- Kh√¥ng c·∫ßn implement m·ªçi th·ª© perfect t·ª´ ƒë·∫ßu: tri·ªÉn khai MVP simple (Sticky Session, Rule-based, JWT).
- Test v·ªõi real users, measure metrics (latency, success rate, cost).
- Polish d·ª±a tr√™n metric (upgrade to Redis Pub/Sub n·∫øu latency issue).
- Dƒ©a ch·ªâ add complexity khi th·ª±c s·ª± c·∫ßn (multi-layer security khi c√≥ regulatory requirement).

**3. Horizontal Scaling ‚â† Vertical Scaling Optimization**

- Scaling t·ªõi 10000 client kh√¥ng ch·ªâ th√™m server (Kubernetes HPA).
- Ph·∫£i redesign t·ª´ng component: stateless servers, shared message broker, distributed auth.
- Optimization ·ªü single server (caching, connection pooling) kh√°c v·ªõi optimization cho scale-out (architecture change).

**4. Monitoring + Observability t·ª´ day 1**

- Kh√¥ng th·ªÉ optimize n·∫øu kh√¥ng bi·∫øt bottleneck ·ªü ƒë√¢u.
- Implement metrics (latency p99, error rate, throughput) t·ª´ MVP.
- Alert khi metric degrade (e.g., latency spike t·ª´ 50ms ‚Üí 200ms).
- Root cause analysis d·ª±a tr√™n trace: request flow qua services, trace latency contribution.

**5. Security ‚â† single-layer defense**

- Defense in depth: multiple layer, if 1 layer fail, still protected by others.
- Trade-off: more layers = more secure, but higher latency, higher complexity.
- Production system: c·∫ßn multi-layer. MVP: c√≥ th·ªÉ skip some layers, nh∆∞ng plan expansion path.

**6. Cost optimization ongoing process**

- MVP cost ~ $150/th√°ng (simple setup, low traffic).
- Production cost ~ $1250/th√°ng (high traffic, redundancy, monitoring).
- But can optimize: use spot instances (save 70%), reserved instances (save 30%), auto-scaling (pay only for used capacity).
- Every optimization = engineer + testing time. Prioritize high-impact optimizations (e.g., Redis cache save 40% DB cost).

---

### K·∫øt Lu·∫≠n

B√†i t·∫≠p n√†y gi√∫p sinh vi√™n:

- Hi·ªÉu **ki·∫øn tr√∫c h·ªá th·ªëng ph√¢n t√°n** trong t√¨nh hu·ªëng th·ª±c t·∫ø (scaling, resilience, security).
- √Åp d·ª•ng **k·ªπ nƒÉng ph√¢n t√≠ch trade-off**: m·ªói solution c√≥ ∆∞u/nh∆∞·ª£c, tradeoff r√µ r√†ng.
- **Thi·∫øt k·∫ø t·ª´ng b∆∞·ªõc**: MVP ‚Üí polish ‚Üí production, incremental improvement.
- **Gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ th·ª±c t·∫ø** m√† engineer g·∫∑p ph·∫£i: l√†m sao scale t·ªõi 1000 users? L√†m sao robust khi website thay ƒë·ªïi? L√†m sao b·∫£o v·ªá user data?

Team ph·∫£i present design, justify trade-offs, explain why solution X better than Y for timeline/budget/risk constraints. ƒê√≥ l√† k·ªπ nƒÉng architecture m√† c√¥ng ty c·∫ßn.

---

## PH√ÇN T√çCH CHI TI·∫æT: METRICS & COST COMPARISON

### Scenario 1: WebSocket Scaling - Metrics Detailed

**Test case: 1000 concurrent clients, 1 price update per second from Binance**

| Metric                              | Sticky Session      | Redis Pub/Sub     | Rendezvous Hashing  |
| ----------------------------------- | ------------------- | ----------------- | ------------------- |
| **P99 Latency (price delivery)**    | 12ms                | 35ms              | 11ms                |
| **Throughput (updates/sec)**        | 950                 | 920               | 945                 |
| **Memory per server**               | 350MB (333 clients) | 50MB (stateless)  | 350MB (333 clients) |
| **Network bandwidth/server**        | 2.5Mbps (broadcast) | 0.8Mbps (pub/sub) | 2.5Mbps (broadcast) |
| **CPU utilization**                 | 65%                 | 55%               | 60%                 |
| **Reconnect time (1 server crash)** | 3-5 sec             | 2-3 sec           | 1-2 sec             |
| **Resilience score**                | 6/10                | 8/10              | 7/10                |

Ph√¢n t√≠ch:

- **Sticky Session**: latency th·∫•p nh·∫•t (pure in-memory), nh∆∞ng memory cao, crash impact l·ªõn (333 client reconnect).
- **Redis Pub/Sub**: latency cao h∆°n (network hop), nh∆∞ng memory stateless, crash impact th·∫•p (client reconnect seamlessly).
- **Rendezvous Hashing**: latency th·∫•p (like sticky), scalability t·ªët (minimal disruption), nh∆∞ng complexity cao.

**Real-world scenario: Scale 1000 ‚Üí 5000 clients**

- Sticky Session: ph·∫£i scale t·ª´ 3 ‚Üí 15 servers, rehash 90% clients ‚Üí 5-10 ph√∫t downtime (unacceptable).
- Redis Pub/Sub: th√™m 12 servers, stateless auto-scale, no downtime, 0-5 sec disruption/pod.
- Rendezvous: th√™m 12 servers, ~25% clients remapping, 30-60 sec disruption.

**Cost breakdown (monthly):**

- Sticky Session: 5 server √ó $10 + 1 LB √ó $5 = $55
- Redis Pub/Sub: 5 server √ó $10 + 1 LB √ó $5 + Redis cluster (3 instance √ó $20) = $130
- Rendezvous: 5 server √ó $10 + 1 LB √ó $5 + service discovery (Consul/etcd √ó $15) = $70

---

### Scenario 2: Crawler - Cost Analysis Deep Dive

**Extraction volume: 50 websites √ó 20 extractions/day = 1000 extractions/day**

**Rule-Based Crawler cost breakdown:**

- Server: 1 √ó c5.large ($0.085/hour) = $61/month
- Database: RDS PostgreSQL t3.small ($0.017/hour) = $12/month
- Storage (SSD): 100GB = $10/month
- Total: $83/month

**LLM-Based Crawler cost breakdown:**

- Server (lighter): 1 √ó t3.small ($0.0208/hour) = $15/month
- OpenAI API: 1000 extraction √ó 20 tokens avg √ó $0.0005/1K = $10/day = $300/month
- Database: RDS PostgreSQL t3.small = $12/month
- Total: $327/month

**Hybrid Crawler cost breakdown:**

- Server: 1 √ó t3.medium ($0.0416/hour) = $30/month
- Database: RDS PostgreSQL t3.small = $12/month
- OpenAI API (fallback): 300 extraction/day √ó 20 tokens √ó $0.0005/1K = $3/day = $90/month
- Total: $132/month

**Cost comparison for 12 months:**

- Rule-Based: $83 √ó 12 = $996
- LLM-Based: $327 √ó 12 = $3,924
- Hybrid: $132 √ó 12 = $1,584

**ROI analysis:**

- Rule-based success rate: 60% (unstable website ‚Üí fail extraction).
- LLM-based success rate: 92% (robust).
- Hybrid success rate: 88% (good balance).

If success rate critical (e.g., sentiment analysis require 85%+ completeness):

- Rule-based only: 60% √ó 1000 extraction = 600 useful data/day.
- Hybrid: 88% √ó 1000 extraction = 880 useful data/day.
- Delta: +280 useful data/day. Worth $1584 - $996 = $588 extra cost? YES if business value of 280 extra sentiment analysis > $588.

---

### Scenario 3: Security - Risk Matrix & Mitigation

**Threat landscape:**

| Threat                        | Impact                      | Probability | Mitigation                              |
| ----------------------------- | --------------------------- | ----------- | --------------------------------------- |
| **User token theft (XSS)**    | Critical (account takeover) | Medium      | CSP, JWT short-lived, token rotation    |
| **API key leak**              | Critical (resource abuse)   | Medium      | Vault (HashiCorp), env isolation, audit |
| **Unauthorized service call** | High (data breach)          | High        | mTLS, S2S JWT, RBAC                     |
| **DDoS attack**               | High (service unavailable)  | Medium      | CloudFlare, rate limiting, auto-scaling |
| **Database breach**           | Critical (all user data)    | Low         | Encryption at rest, TLS in transit      |
| **Insider threat**            | Medium                      | Low         | Audit logging, RBAC, separation of duty |

**Cost of security incidents (industry average):**

- DDoS attack (4 hour downtime): Revenue loss $50k + remediation $5k = $55k
- Data breach (10000 user records): Notification + recovery $100k + reputation damage $500k = $600k
- Unauthorized access (compromised service): Incident response $20k + fix + retest $30k = $50k

**Security investment ROI:**

- Investment: $200/month √ó 12 = $2400/year (multi-layer security)
- Expected loss reduction: prevent 1 DDoS (save $55k) + reduce breach risk 30% (reduce $600k exposure to $420k) = $235k/year
- ROI: $235k / $2400 = 97.9x (excellent)

---

### Timeline & Resource Planning

**Project timeline: 2 months (8 weeks)**

**Week 1-2: Setup + WebSocket MVP**

- Setup Kubernetes cluster (local or cloud)
- Implement basic WebSocket server (Node.js + Socket.io)
- Sticky session load balancer (Nginx)
- Test: 100 concurrent clients
- Team: 2 engineer

**Week 3-4: Crawler MVP**

- Rule-based crawler for 10 stable websites
- BeautifulSoup/lxml for HTML parsing
- Store results to database
- Test: 95%+ extraction success rate
- Team: 1 engineer

**Week 5: Security Foundation**

- JWT authentication
- API Gateway (basic rate limiting)
- RBAC role definition
- Test: login, token rotation, unauthorized access denied
- Team: 1 engineer

**Week 6: Polish & Enhancement**

- Redis Pub/Sub for WebSocket (if latency issue)
- Hybrid crawler (fallback to LLM)
- Zero-Trust service auth (mTLS)
- Team: 2 engineer

**Week 7-8: Testing & Documentation**

- Load test: 1000 concurrent clients
- Failure scenario test: service crash, network partition, API rate limit
- Documentation: architecture diagram, deployment guide, operation manual
- Team: 2-3 engineer

**Total effort: ~15-20 engineer-weeks**
**Team size: 3-4 engineer**
**Timeline feasibility: YES (8 weeks realistic)**

---

### Implementation Roadmap

**Phase 1 (Week 1-4): MVP Delivery**

- Goal: Functional system with basic scaling (100 clients), basic crawler (10 sites), basic auth.
- Success metric: Demo to stakeholder, gather feedback.
- Deliverable: Working prototype, architecture diagram.

**Phase 2 (Week 5-7): Enhancement & Scale Test**

- Goal: Scale to 1000 clients, 50 websites, implement security layers.
- Success metric: Load test pass (1000 client, <50ms latency), crawler success 85%+.
- Deliverable: Production-ready codebase, monitoring setup, runbook.

**Phase 3 (Week 8+): Optimization & Hardening**

- Goal: Optimize latency, cost, reliability based on metrics.
- Consider: migrate to Redis Pub/Sub, add LLM fallback, implement multi-layer security.
- Success metric: Meet SLA (99.9% uptime, <100ms latency p99), cost < $1500/month.
- Deliverable: Production deployment, incident playbook, capacity planning.

---

### Lessons Learned & Best Practices

**1. Architecture Decision Record (ADR)**
Document every major decision: why we choose Sticky Session (not Redis) for MVP, trade-offs considered, assumptions.
When requirement change (e.g., need 5000 clients), can review ADR, understand previous context, make informed decision to migrate.

**2. Monitoring from Day 1**
Implement basic metrics (latency, error rate, throughput) in MVP. As system grows, add more detailed metrics (service-to-service latency, crawler success rate, security event rate).
Without metrics, cannot optimize. With metrics, can identify bottleneck early.

**3. Test realistic scenarios**
Not just happy path: test service crash, network delay, API rate limit. Simulate chaos: kill random service, introduce network latency.
Chaos engineering: inject failures intentionally, ensure system recover gracefully.

**4. Gradual migration strategy**
When migrate sticky session ‚Üí Redis Pub/Sub: run both in parallel (canary deployment). Route 10% traffic to Redis, 90% to sticky session.
Monitor metric difference: if Redis latency worse, rollback. If good, increase ratio gradually.

**5. Cost optimization iterative**
Start with simple setup (cheaper but less optimal). As traffic grow, re-evaluate: is current infra cost-efficient?
Example: after 6 months, analyze: "how much we paying for cold capacity?" ‚Üí use auto-scaling to reduce.

---

## BI·ªÇU ƒê·ªí KI·∫æN TR√öC T·ªîNG H·ª¢P (ASCII Diagram)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        CLIENT LAYER                                 ‚îÇ
‚îÇ  Web Browser (Chrome/Firefox) | Mobile App (iOS/Android)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     EDGE LAYER                                      ‚îÇ
‚îÇ  CloudFlare (DDoS mitigation, cache, WAF)                          ‚îÇ
‚îÇ  Geo-routing (latency optimization)                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   LOAD BALANCER LAYER                               ‚îÇ
‚îÇ  Nginx/HAProxy                                                      ‚îÇ
‚îÇ  IP Hash (Sticky Session) for WebSocket                            ‚îÇ
‚îÇ  Rate Limiting: 1000 req/min/IP                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                ‚îÇ                ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Service  ‚îÇ   ‚îÇ  Service       ‚îÇ   ‚îÇService ‚îÇ
   ‚îÇ Replica  ‚îÇ   ‚îÇ  Replica       ‚îÇ   ‚îÇReplica ‚îÇ
   ‚îÇ 1        ‚îÇ   ‚îÇ  2             ‚îÇ   ‚îÇ3       ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                  ‚îÇ                 ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                  ‚îÇ                  ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Data     ‚îÇ      ‚îÇ  Message   ‚îÇ    ‚îÇ Cache   ‚îÇ
   ‚îÇ Storage  ‚îÇ      ‚îÇ  Broker    ‚îÇ    ‚îÇ (Redis) ‚îÇ
   ‚îÇ(PostgreSQL)    ‚îÇ (Redis      ‚îÇ    ‚îÇ         ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ Pub/Sub)   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Microservices detail:
- API Gateway (auth, rate limit, routing)
- WebSocket Gateway (long-lived connection, broadcast)
- Price Service (Binance integration, price update)
- Crawler Service (news extraction, hybrid approach)
- Analytics Service (sentiment analysis, metrics)
- Auth Service (JWT, token rotation)
- Audit Service (logging, compliance)
```

---

## CASE STUDY: REAL-WORLD DEPLOYMENT SCENARIO

### Context: Launch to Production (100k users in 6 months)

**Th√°ng 1-2 (0-25k users): MVP Phase**

- Deployment: 1 datacenter, 3 WebSocket server (sticky session), 1 crawler server, 1 auth service.
- Expected growth: 100 ‚Üí 500 ‚Üí 2000 users/day.
- Bottleneck anticipated: WebSocket latency when spike (rush hour 6-9pm).

**Metrics during MVP phase:**

- P99 latency: 45-60ms (target <50ms but acceptable for MVP).
- Availability: 99.5% (few crashes due to resource constraint).
- Crawler success rate: 65% (rule-based, some website structure change).
- Cost: $500/month (bare minimum infrastructure).

**Decision point (Week 8):**

- User feedback: "Price update delays during peak hours" ‚Üí latency issue confirmed.
- Metrics show: CPU spike to 85% during 7-9pm, latency jumps to 200ms.
- Options:
  - Option A: Upgrade to bigger instance ($1000/month) ‚Üí quick fix, but not scalable.
  - Option B: Migrate to Redis Pub/Sub ($800/month) ‚Üí more latency initially but scalable.
  - Option C: Implement Rendezvous Hashing ($600/month) ‚Üí complex but best latency.
- **Decision**: Go with Option B (Redis Pub/Sub) because MVP phase, team unfamiliar with Rendezvous Hashing, Redis simpler to operate.

**Th√°ng 3-4 (25k-60k users): Growth Phase**

- Deployment: 1 primary DC + 1 backup DC (active-passive for HA).
- Infrastructure:
  - WebSocket: 10 servers (Redis Pub/Sub, stateless).
  - Crawler: 3 servers (hybrid approach, rule+LLM).
  - Auth: 2 servers (JWT, token rotation).
  - Supporting: Redis cluster (3 master), PostgreSQL (1 primary + 1 replica).

**Incident during growth phase:**

- At Week 11 (55k concurrent users, peak hour 7:30pm):
  - Redis master crash (out of memory, redis-cli flushdb accidentally executed by ops engineer).
  - All WebSocket connections cannot publish price update (pub/sub blocked).
  - 55k users see "Connection lost" message.
  - Client auto-reconnect with exponential backoff ‚Üí all 55k reconnect in 30-60 seconds.
  - RTO (Recovery Time Objective): 2 minutes (redis restart + client reconnect).
  - RPO (Recovery Point Objective): 0 (pub/sub is transient, no persistent data lost).

**Post-incident action:**

- Immediate: Add better monitoring (Redis memory threshold alert at 80%, auto-trigger paging).
- Short-term: Implement RBAC (only specific ops can execute flushdb).
- Medium-term: Setup Redis cluster with proper failover (master-slave with sentinel).
- Lessons: Human error most common cause. Monitoring + access control prevent 90% of incidents.

**Th√°ng 5-6 (60k-100k users): Scale & Optimize Phase**

- Target: 99.99% availability (4 nines), P99 latency <30ms.
- Deployment strategy:
  - Multi-region: Deploy to 3 datacenters (US-East, EU, AP-Southeast).
  - Global load balancing: Route user to nearest datacenter (geo-latency optimization).
  - Database: Read replica in each region (local read, latency <5ms).
  - Cache: Redis cluster replicated across regions.

**Challenges at scale:**

1. **Data consistency**: Price update in US-East published, but user in EU-Southeast may see 200ms delay (time to replicate). Solution: Accept eventual consistency, show timestamp to user ("price as of 2 sec ago").
2. **Crawler scalability**: 50 websites, 100k users √ó 5 portfolio track/user = 500k sentiment data/day. Single crawler server cannot handle. Solution: Shard crawler: server 1 crawl news (20 sites), server 2 crawl (20 sites), server 3 crawl (10 sites). Results merge before analytics.
3. **Cost explosion**: At 100k users, infrastructure cost $5k/month. Business model (freemium 70% users, pro 30% users at $10/month) = $30k/month revenue. Gross margin still okay but need optimize.

**Cost optimization tactics:**

- Use spot instances: 70% discount. WebSocket services tolerant to interruption (client auto-reconnect). Use spot for 70% capacity, reserved for 30% base load.
- Database optimization: Implement connection pooling (reduce per-query overhead). Denormalize some data (trade storage for query speed).
- Cache aggressive: Redis cache price data. Hit ratio 95%+ ‚Üí DB query reduce 95%. Cost reduction: $2k/month (less DB read).

---

## CHUY√äN ƒê·ªÄ N√ÇNG CAO: PERFORMANCE TUNING & BOTTLENECK ANALYSIS

### Identifying Bottleneck (Methodology)

**Step 1: Measure end-to-end latency**

- User action: "request price update" ‚Üí Server respond.
- Measure p50, p95, p99 latency (not just average).
- Example: p50=20ms (50% request), p99=150ms (1% slow request).
- If p99=150ms > SLA (target 50ms) ‚Üí bottleneck exist.

**Step 2: Trace request path**

- Add timestamps at each component:
  - T0: Request enter API Gateway
  - T1: API Gateway forward to WebSocket Service
  - T2: WebSocket Service publish to Redis
  - T3: Redis broadcast to 3 servers
  - T4: WebSocket Service broadcast to clients
  - T5: Client receive (React re-render)
- Calculate contribution: T1-T0, T2-T1, T3-T2, T4-T3, T5-T4.
- If T3-T2 (Redis) is 100ms out of 150ms total ‚Üí Redis is bottleneck.

**Step 3: Profile bottleneck component**

- If Redis bottleneck: profile Redis commands (INFO COMMANDSTATS shows command stats).
- If WebSocket bottleneck: profile broadcast loop (how many connection iterate, how long per connection).
- Use tools: strace (system call trace), perf (CPU profiling), ab/wrk (load testing).

**Step 4: Root cause analysis**

- Example: Redis slow because 1M keys in memory (memory pressure, eviction overhead).
- Solution: Enable Redis memory compression, or shard Redis (separate instance per domain).
- Re-test: measure latency improvement after fix.

### Common Performance Anti-Pattern & Fix

**Anti-pattern 1: Synchronous service call in request path**

```
User request ‚Üí API Gateway ‚Üí Price Service (wait) ‚Üí Crawler Service (wait) ‚Üí Analytics Service (wait)
Total latency: sum of all service latency (e.g., 10+20+30=60ms)
```

Fix: Use async/queue. Price Service emit event ‚Üí other services consume async (parallel, not sequential).

**Anti-pattern 2: N+1 query problem**

```
Get user portfolio (1 query) ‚Üí for each stock, get price (N query) ‚Üí total N+1
If user has 100 stocks ‚Üí 101 query, very slow.
```

Fix: Use batch query or JOIN (1 query, fetch all at once).

**Anti-pattern 3: No caching**

```
Every request hit database ‚Üí database overload, latency high.
```

Fix: Redis cache frequently accessed data (price, user profile). Hit ratio 80%+ ‚Üí DB load reduce 80%.

**Anti-pattern 4: Unbounded connection pool**

```
Service A create connection to Service B on demand ‚Üí if 1000 request ‚Üí 1000 connection.
Service B reach connection limit (e.g., PostgreSQL max_connection=100) ‚Üí new connection fail.
```

Fix: Connection pool with max size (e.g., pool_size=50). Excess request queue, wait for available connection.

---

## APPENDIX: TECHNICAL REFERENCE & TOOLS

### Required Technologies

**Infrastructure:**

- Cloud provider: AWS (EC2, RDS, ElastiCache) or GCP (Compute Engine, Cloud SQL, Memorystore).
- Container: Docker (containerize services).
- Orchestration: Kubernetes (manage container scaling, networking).
- CI/CD: GitHub Actions or GitLab CI (automate build, test, deploy).

**Backend:**

- Language: Python (crawler), Node.js (WebSocket service), Go (high-performance service).
- Framework: Express.js (REST API), Socket.io (WebSocket), FastAPI (Python API).
- Database: PostgreSQL (primary DB), Redis (cache + pub/sub).
- Message queue: Redis Pub/Sub (for MVP), Kafka (for high-volume events).

**Frontend:**

- Framework: React or Vue.js (SPA).
- State management: Redux (React) or Vuex (Vue).
- Real-time: Socket.io client library.

**Monitoring & Observability:**

- Metrics: Prometheus (collect), Grafana (visualize).
- Logging: ELK stack (Elasticsearch, Logstash, Kibana) or Loki.
- Tracing: Jaeger or Zipkin (distributed tracing).
- Alerting: Prometheus AlertManager or PagerDuty.

### Key Metrics to Track

**Application metrics:**

- Request latency (p50, p95, p99)
- Error rate (% failed request)
- Throughput (request/sec)
- Success rate (for crawler: % data extracted successfully)

**Infrastructure metrics:**

- CPU utilization
- Memory utilization
- Disk I/O
- Network bandwidth
- Pod restart count

**Business metrics:**

- Daily active user (DAU)
- User retention (day 7, day 30)
- Revenue per user
- System availability (% uptime)

### Recommended Reading & Resources

- "Designing Data-Intensive Applications" by Martin Kleppmann (system design fundamentals)
- "Release It!" by Michael Nygard (operational resilience patterns)
- AWS Architecture Best Practices: https://aws.amazon.com/architecture/
- Kubernetes documentation: https://kubernetes.io/docs/
- Redis commands documentation: https://redis.io/commands

---

## C√ÇUH·ªéI T·ª∞ ƒê√ÅNH GI√Å

Sau khi ho√†n th√†nh b√°o c√°o, h√£y t·ª± tr·∫£ l·ªùi c√°c c√¢u h·ªèi sau:

1. **Scenario 1 - WebSocket**: N·∫øu client t·ª´ 1000 ‚Üí 10000 (10x), c√°ch n√†o scale? Chi ph√≠ tƒÉng bao nhi√™u?

   - Expected: Nh·ªõ redis pub/sub d·ªÖ scale ngang, cost tƒÉng ~3-4x (need larger redis cluster, more servers).

2. **Scenario 2 - Crawler**: Website XYZ thay ƒë·ªïi HTML structure, rule-based fail. L√†m sao nhanh ch√≥ng fix?

   - Expected: D√πng LLM fallback (trong hybrid approach), kh√¥ng c·∫ßn code deploy. Ho·∫∑c manual update selector config.

3. **Scenario 3 - Security**: Token c·ªßa user A b·ªã leak, attacker c√≥ th·ªÉ l√†m g√¨? Bao l√¢u token invalid?

   - Expected: Access token valid 15 min ‚Üí attacker c√≥ 15 min window. Sau ƒë√≥ c·∫ßn refresh token. User c√≥ th·ªÉ logout (add token to blacklist) ‚Üí attacker kh√¥ng th·ªÉ d√πng token ƒë√≥.

4. **Trade-off**: Chi ph√≠ so Sticky Session ($55) vs Redis Pub/Sub ($130) = $75 th√™m. Worth ch∆∞a?

   - Expected: Depends on business value. N·∫øu latency <50ms critical ‚Üí worth ($75 solve latency issue). N·∫øu cost critical ‚Üí not worth.

5. **Architecture**: N·∫øu budget b·ªã c·∫Øt 50%, l√†m sao scale cost efficiency?
   - Expected: D√πng spot instance (70% discount), aggressively cache (reduce DB query), accept lower latency SLA, reduce redundancy (1 region thay v√¨ 3).

---

## K·∫æT LU·∫¨N

ƒê·ªì √°n n√†y y√™u c·∫ßu sinh vi√™n ph√¢n t√≠ch 3 v·∫•n ƒë·ªÅ ki·∫øn tr√∫c ph√¢n t√°n th·ª±c t·∫ø, propose gi·∫£i ph√°p, so s√°nh trade-off. Kh√¥ng c√≥ "perfect" solution, ch·ªâ c√≥ "tradeoff yang smart".

**K·ªπ nƒÉng r√∫t ra:**

- System design thinking: t·ª´ y√™u c·∫ßu ‚Üí architecture ‚Üí metric.
- Trade-off analysis: cost vs performance, complexity vs reliability, etc.
- Operational mindset: kh√¥ng ch·ªâ build, ph·∫£i maintain, monitor, optimize.
- Communication: gi·∫£i th√≠ch why choice X, not Y, convince team and stakeholder.

**·ª®ng d·ª•ng th·ª±c t·∫ø:** C√°c k·ªπ nƒÉng n√†y applicable cho m·ªçi large-scale system: e-commerce, social media, streaming platform, fintech. Principles gi·ªëng nhau, implementation differ.

**Presentation tips:**

- Focus on WHY, not implementation detail. Reviewer mu·ªën hi·ªÉu t∆∞ duy, kh√¥ng mu·ªën xem code.
- Use diagram liberally. Architecture th∆∞·ªùng visualize t·ªët h∆°n text.
- Give context: "at 1000 client, we see X problem, so we choose Y solution".
- Discuss trade-off: "we could do Z, but we chose Y because [reason]".

---

## H∆Ø·ªöNG D·∫™N TRI·ªÇN KHAI CHI TI·∫æT (IMPLEMENTATION GUIDE)

### Phase 1: Setup Local Development Environment (Week 1)

**Prerequisites:**

- Docker, Docker Compose installed
- Node.js 18+ for WebSocket service
- Python 3.10+ for crawler service
- Git for version control

**Step 1: Initialize project structure**

```
crypto-analysis/
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ websocket-gateway/     # Node.js + Socket.io
‚îÇ   ‚îú‚îÄ‚îÄ price-service/          # Data ingestion from Binance
‚îÇ   ‚îú‚îÄ‚îÄ crawler-service/        # Python crawler (rule+LLM hybrid)
‚îÇ   ‚îú‚îÄ‚îÄ auth-service/           # JWT token service
‚îÇ   ‚îî‚îÄ‚îÄ api-gateway/            # Rate limiting, auth check
‚îú‚îÄ‚îÄ infrastructure/
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml      # Local dev setup
‚îÇ   ‚îú‚îÄ‚îÄ kubernetes/             # Production Kubernetes manifests
‚îÇ   ‚îî‚îÄ‚îÄ terraform/              # IaC for cloud infrastructure
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îú‚îÄ‚îÄ schema.sql              # PostgreSQL schema
‚îÇ   ‚îî‚îÄ‚îÄ migrations/             # Database migrations
‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yml          # Prometheus config
‚îÇ   ‚îî‚îÄ‚îÄ grafana-dashboards/     # Grafana dashboard JSON
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ architecture.md
    ‚îî‚îÄ‚îÄ deployment.md
```

**Step 2: Docker Compose for local development**
File: docker-compose.yml

```
version: '3.8'
services:
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_PASSWORD: dev_password
      POSTGRES_DB: crypto_analysis
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/schema.sql:/docker-entrypoint-initdb.d/schema.sql

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes

  websocket-gateway:
    build: ./services/websocket-gateway
    ports:
      - "3000:3000"
    environment:
      REDIS_URL: redis://redis:6379
      DATABASE_URL: postgresql://postgres:dev_password@postgres:5432/crypto_analysis
    depends_on:
      - redis
      - postgres

  api-gateway:
    build: ./services/api-gateway
    ports:
      - "8000:8000"
    environment:
      REDIS_URL: redis://redis:6379
      DATABASE_URL: postgresql://postgres:dev_password@postgres:5432/crypto_analysis
    depends_on:
      - redis
      - postgres

volumes:
  postgres_data:
```

Run: `docker-compose up -d` ‚Üí all services start locally.

**Step 3: Test connectivity**

```
# Test Redis
docker-compose exec redis redis-cli PING

# Test PostgreSQL
docker-compose exec postgres psql -U postgres -d crypto_analysis -c "SELECT * FROM users;"

# Test WebSocket
curl http://localhost:3000/health

# Test API Gateway
curl http://localhost:8000/health
```

---

### Phase 2: Implement Core Services (Week 2-3)

**WebSocket Gateway Implementation (Node.js + Socket.io + Redis Pub/Sub)**

Key components:

1. Connection manager: track connected clients, generate unique client_id.
2. Message handler: receive price update, broadcast to clients.
3. Redis subscriber: subscribe to price-update channel, relay to connected clients.
4. Health check: /health endpoint, return {"status": "healthy", "connected_clients": 1000}.

Pseudo-code flow:

```
1. Client connect (WebSocket handshake)
   ‚Üí generate client_id
   ‚Üí subscribe to price-update channel (Redis)
   ‚Üí send welcome message to client

2. Receive price update from Binance API
   ‚Üí validate price data
   ‚Üí publish to Redis channel (price-update)

3. Redis broadcast to all gateway subscribers
   ‚Üí each gateway receive message
   ‚Üí iterate connected clients
   ‚Üí broadcast message via WebSocket

4. Client disconnect
   ‚Üí cleanup connection
   ‚Üí send disconnect event (optional: log to database for analytics)
```

Metrics to implement:

- Connected clients gauge (current count)
- Message latency histogram (publish time ‚Üí client receive time)
- Connection duration (how long client stay connected)
- Error rate (failed broadcast, disconnect unexpected)

**Crawler Service Implementation (Python + BeautifulSoup/Selenium + OpenAI)**

Key components:

1. Rule-based extractor: XPath/CSS selector config per website.
2. LLM fallback: call OpenAI API if rule-based fail.
3. Scheduler: run crawler every 5 minutes per website.
4. Storage: insert extracted news to database + cache.

Pseudo-code flow:

```
1. For each website in config:
   a. Fetch HTML (with user-agent, proxy rotation)
   b. Try rule-based extraction (XPath/CSS)
      - If success & confidence > 70% ‚Üí save to DB, done
      - If fail or confidence < 70% ‚Üí goto step 2

   c. Try LLM extraction (OpenAI API)
      - If success ‚Üí save to DB, done
      - If fail ‚Üí save error log, mark as "manual_review"

2. For failed extraction:
   - Log error (website, timestamp, error type)
   - Alert ops team to manual investigate
   - Update rules if needed

3. Metrics:
   - Rule-based success rate per website
   - LLM success rate
   - Average extraction latency
   - Error rate
```

**Auth Service Implementation (JWT + Token Rotation)**

Key components:

1. Password verification: hash user password, verify on login.
2. Token issuance: create access_token (15 min expiry) + refresh_token (7 day expiry).
3. Token verification: verify signature, check expiry.
4. Token rotation: on refresh, invalidate old refresh_token, issue new one.

Pseudo-code flow:

```
1. Login endpoint
   - Receive username, password
   - Hash password, compare with DB
   - If match: create access_token + refresh_token, return
   - If fail: return 401 Unauthorized

2. Protected endpoint (e.g., /user/portfolio)
   - Receive request + access_token in Authorization header
   - Verify token signature (using secret key)
   - Check expiry: if expired ‚Üí return 401
   - Extract user_id from token claims
   - Check permission (RBAC): does user have permission? If yes ‚Üí process request

3. Refresh endpoint
   - Receive refresh_token
   - Verify token signature, check expiry
   - If valid: create new access_token + new refresh_token, return
   - Add old refresh_token to blacklist (Redis) to prevent reuse

4. Logout endpoint
   - Receive access_token
   - Add to blacklist (Redis TTL = token expiry time)
   - Return success
```

---

### Phase 3: Monitoring & Observability Setup (Week 4)

**Prometheus metrics export:**

Each service expose /metrics endpoint:

```
# HELP websocket_connected_clients Current number of connected clients
# TYPE websocket_connected_clients gauge
websocket_connected_clients 1000

# HELP message_latency_ms Message publish to delivery latency in milliseconds
# TYPE message_latency_ms histogram
message_latency_ms_bucket{le="10"} 800
message_latency_ms_bucket{le="50"} 950
message_latency_ms_bucket{le="100"} 990
message_latency_ms_bucket{le="+Inf"} 1000
message_latency_ms_sum 25000
message_latency_ms_count 1000

# HELP crawler_success_rate Crawler extraction success rate per website
# TYPE crawler_success_rate gauge
crawler_success_rate{website="cointelegraph"} 0.95
crawler_success_rate{website="theblock"} 0.88
crawler_success_rate{website="medium"} 0.72
```

**Grafana dashboard:**

- Graph 1: Connected clients trend (time series)
- Graph 2: Message latency p50, p95, p99 (separate lines)
- Graph 3: Error rate per service
- Graph 4: Crawler success rate per website
- Graph 5: Database connection pool utilization
- Graph 6: Redis memory usage

**Alerting rules (Prometheus AlertManager):**

- Alert if connected_clients > 500 (capacity planning)
- Alert if message_latency_p99 > 100ms (performance degradation)
- Alert if error_rate > 1% (error spike)
- Alert if redis_memory_used > 80% (memory pressure)
- Alert if service_down (health check fail)

---

### Phase 4: Testing Strategy (Week 5-6)

**Unit tests:**

- Token verification (valid/invalid/expired token)
- Price parsing (correct extraction from Binance API)
- XPath selector matching (test multiple websites)

**Integration tests:**

- WebSocket connection ‚Üí message publish ‚Üí delivery
- Auth flow: login ‚Üí get token ‚Üí call protected endpoint
- Crawler: fetch website ‚Üí extract ‚Üí store DB

**Load tests:**

- Tool: Apache JMeter or wrk
- Scenario: 1000 concurrent WebSocket clients, 1 price update/sec
- Measure: latency p50/p95/p99, error rate, throughput
- Target: p99 < 50ms, error rate < 0.1%

**Chaos tests:**

- Kill 1 service ‚Üí system recover? In how long?
- Network delay +500ms ‚Üí latency impact? Message loss?
- Database failure ‚Üí graceful degrade?

Example JMeter test:

```
Thread group: 1000 concurrent users
Ramp-up: 30 sec (add 33 users/sec)
Duration: 5 minutes
Request: WebSocket connect to ws://localhost:3000

Assertion: expect message arrive within 50ms
Report: histogram of latency, error count
```

---

### Phase 5: Production Deployment (Week 7-8)

**Kubernetes deployment manifest (websocket-gateway):**

File: services/websocket-gateway/k8s/deployment.yaml

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: websocket-gateway
spec:
  replicas: 3  # 3 instance for HA
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: websocket-gateway
  template:
    metadata:
      labels:
        app: websocket-gateway
    spec:
      containers:
      - name: websocket-gateway
        image: myregistry.azurecr.io/websocket-gateway:v1.0.0
        ports:
        - containerPort: 3000
        env:
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: redis-secret
              key: url
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: url
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 10
          periodSeconds: 5
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: websocket-gateway
spec:
  selector:
    app: websocket-gateway
  ports:
  - port: 80
    targetPort: 3000
  type: LoadBalancer  # Expose via public IP
---
apiVersion: autoscaling.k8s.io/v2
kind: HorizontalPodAutoscaler
metadata:
  name: websocket-gateway-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: websocket-gateway
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Scale up if CPU > 70%
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80  # Scale up if memory > 80%
```

Deploy:

```
kubectl apply -f services/websocket-gateway/k8s/deployment.yaml
```

Monitor deployment:

```
kubectl get pods -l app=websocket-gateway  # View pod status
kubectl logs -l app=websocket-gateway --tail=100  # View recent logs
kubectl describe deployment websocket-gateway  # View deployment status
```

---

## TROUBLESHOOTING GUIDE

### Common Issues & Solutions

**Issue 1: WebSocket latency spike during peak hours**

- Symptom: p99 latency jump from 50ms ‚Üí 200ms at 6-9pm
- Root cause: CPU spike (100% utilization), broadcast loop slow
- Investigation:
  - Check CPU usage (kubectl top pods)
  - Check Redis latency (redis-cli --latency)
  - Check network bandwidth (check if saturated)
- Solution:
  - Scale up: add more WebSocket gateway pods
  - Optimize: reduce message size, batch broadcast
  - Cache: pre-compute frequently accessed data

**Issue 2: Crawler success rate drop from 95% ‚Üí 60%**

- Symptom: Many news extraction fail
- Root cause: Website HTML structure change (happens weekly)
- Investigation:
  - Analyze failed extraction logs: which website, what error
  - Manually fetch website, compare with expected HTML structure
  - Check if website doing rate limiting (429 Too Many Request)
- Solution:
  - Update XPath selector (if rule-based fail)
  - Decrease request frequency (to avoid rate limit)
  - Add LLM fallback (hybrid approach)

**Issue 3: Database connection pool exhausted**

- Symptom: Application hang, cannot query database
- Root cause: Too many open connections, not enough pool size
- Investigation:
  - Check database connection count (SELECT count(\*) FROM pg_stat_activity;)
  - Check pool size configuration
  - Check if there are long-running queries (blocking connections)
- Solution:
  - Increase pool size (if infrastructure allow)
  - Optimize slow queries (add index, rewrite query)
  - Close idle connections (set connection timeout)

**Issue 4: Redis memory exhausted, eviction happen**

- Symptom: Random key deletion, cache miss increase
- Root cause: Too much data in Redis, exceed memory limit
- Investigation:
  - Check Redis memory usage (INFO memory)
  - Check which keys consume most memory (MEMORY DOCTOR)
  - Check if key expiry is working (should auto-cleanup expired key)
- Solution:
  - Increase Redis memory limit
  - Reduce TTL (time-to-live) for cache entries
  - Shard Redis (separate instance per domain)

**Issue 5: JWT token verification fail for some user**

- Symptom: User get 401 Unauthorized randomly
- Root cause: Token signature mismatch (using wrong secret key)
- Investigation:
  - Check if service using same secret key for verify (should be shared)
  - Check if secret key rotated recently
  - Check if token tampered (manual decode JWT to verify signature)
- Solution:
  - Ensure all services use same secret key (store in secure vault)
  - Implement key rotation gracefully (accept old + new key during transition)

---

## APPENDIX: DETAILED ARCHITECTURE DIAGRAMS

### Diagram 1: Problem 1 - WebSocket Scaling Without vs With Redis Pub/Sub

S∆° ƒë·ªì chi ti·∫øt so s√°nh ki·∫øn tr√∫c WebSocket scaling:

**Without Pub/Sub (Single Server - ‚ùå Kh√¥ng kh·∫£ thi):**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Price Service                               ‚îÇ
‚îÇ (Nh·∫≠n gi√° t·ª´ Binance)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ WebSocket Server 1    ‚îÇ
         ‚îÇ (1 server duy nh·∫•t)   ‚îÇ
         ‚îÇ                       ‚îÇ
         ‚îÇ 10,000 connections    ‚îÇ
         ‚îÇ Memory: 10-50GB       ‚îÇ
         ‚îÇ CPU: 100% (bottleneck)‚îÇ
         ‚îÇ                       ‚îÇ
         ‚îÇ ‚ùå Cannot scale       ‚îÇ
         ‚îÇ ‚ùå Single SPOF        ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

Problem: N·∫øu server crash ‚Üí 10K users m·∫•t k·∫øt n·ªëi. Kh√¥ng th·ªÉ th√™m server v√¨ connections "d√≠nh" v√†o server n√†y.

**With Redis Pub/Sub (Distributed - ‚úÖ Kh·∫£ thi):**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Price Service                        ‚îÇ
‚îÇ (Nh·∫≠n 1 l·∫ßn t·ª´ Binance)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº PUBLISH to Redis
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Redis Pub/Sub Broker   ‚îÇ
    ‚îÇ (Central Hub)          ‚îÇ
    ‚îÇ                        ‚îÇ
    ‚îÇ Channel: btcusdt:price ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ        ‚îÇ      ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  WS   ‚îÇ ‚îÇ  WS   ‚îÇ ‚îÇ  WS   ‚îÇ ‚îÇ   WS    ‚îÇ
    ‚îÇ Srv 1 ‚îÇ ‚îÇ Srv 2 ‚îÇ ‚îÇ Srv 3 ‚îÇ ‚îÇ Srv N   ‚îÇ
    ‚îÇ 2,500 ‚îÇ ‚îÇ 2,500 ‚îÇ ‚îÇ 2,500 ‚îÇ ‚îÇ 2,500   ‚îÇ
    ‚îÇ conn  ‚îÇ ‚îÇ conn  ‚îÇ ‚îÇ conn  ‚îÇ ‚îÇ conn    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    ‚úÖ Stateless servers
    ‚úÖ Can scale to N servers
    ‚úÖ Each server independent
    ‚úÖ Latency: <100ms
```

Benefit: Th√™m server ‚Üí t·ª± ƒë·ªông load balance, kh√¥ng c·∫ßn redeploy. Stateless = Kubernetes HPA auto-scale.

---

### Diagram 2: Problem 2 - Crawler Self-Recovery from HTML Changes

S∆° ƒë·ªì crawler ph√°t hi·ªán v√† t·ª± recovery khi website thay ƒë·ªïi HTML:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Website XYZ                         ‚îÇ
‚îÇ (Thay ƒë·ªïi HTML structure)           ‚îÇ
‚îÇ Old: class='headline'               ‚îÇ
‚îÇ New: class='article-h1'             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Rule-Based Extractor       ‚îÇ
    ‚îÇ (Try XPath/CSS selector)   ‚îÇ
    ‚îÇ                            ‚îÇ
    ‚îÇ Query: .headline           ‚îÇ
    ‚îÇ Result: ‚ùå No match        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Pattern Cache          ‚îÇ
    ‚îÇ (Check known patterns) ‚îÇ
    ‚îÇ                        ‚îÇ
    ‚îÇ Known: .headline ‚úì     ‚îÇ
    ‚îÇ New: .article-h1 ‚ùå    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ Pattern not found
             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ LLM Fallback Trigger   ‚îÇ
    ‚îÇ (Intelligent recovery) ‚îÇ
    ‚îÇ                        ‚îÇ
    ‚îÇ Send: Raw HTML + prompt‚îÇ
    ‚îÇ "Find article title"   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ GPT-4 Analysis         ‚îÇ
    ‚îÇ                        ‚îÇ
    ‚îÇ Output:                ‚îÇ
    ‚îÇ - New selector: .h1    ‚îÇ
    ‚îÇ - Confidence: 0.92     ‚îÇ
    ‚îÇ - Cost: $0.10          ‚îÇ
    ‚îÇ - Time: 3 seconds      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Validation & Cache     ‚îÇ
    ‚îÇ                        ‚îÇ
    ‚îÇ Test on 3 pages:       ‚îÇ
    ‚îÇ Success rate: 90%+ ‚úì   ‚îÇ
    ‚îÇ                        ‚îÇ
    ‚îÇ Store new selector in  ‚îÇ
    ‚îÇ cache for next crawl   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Monitor & Alert        ‚îÇ
    ‚îÇ                        ‚îÇ
    ‚îÇ Log: "HTML change      ‚îÇ
    ‚îÇ detected & recovered"  ‚îÇ
    ‚îÇ                        ‚îÇ
    ‚îÇ Alert ops: Review      ‚îÇ
    ‚îÇ for manual verification‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

Key benefit: Hybrid approach automatically adapt khi website thay ƒë·ªïi, kh√¥ng c·∫ßn manual code redeploy.

---

### Diagram 3: Problem 3 - Multi-Layer Security Defense

S∆° ƒë·ªì c√°c layer b·∫£o m·∫≠t t·ª´ edge ‚Üí microservice ‚Üí database:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ User Request                                             ‚îÇ
‚îÇ (Potential attacker)                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Layer 1: Edge (CloudFlare)     ‚îÇ
        ‚îÇ - DDoS mitigation              ‚îÇ
        ‚îÇ - Block malicious IP           ‚îÇ
        ‚îÇ - WAF rules                    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚ùå Attacker blocked here 90% of time
                    ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Layer 2: Rate Limiter (Nginx) ‚îÇ
        ‚îÇ - Max 1000 req/min/IP         ‚îÇ
        ‚îÇ - Detect brute force          ‚îÇ
        ‚îÇ - 429 Too Many Requests       ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚ùå Attacker blocked here 5% of time
                    ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Layer 3: API Gateway             ‚îÇ
        ‚îÇ - JWT token verification         ‚îÇ
        ‚îÇ - Check token expiry             ‚îÇ
        ‚îÇ - Extract user_id + permissions ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚ùå Attacker blocked here 4% of time
                    ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Layer 4: RBAC (Authorization)    ‚îÇ
        ‚îÇ - Check user role                ‚îÇ
        ‚îÇ - Check resource permission      ‚îÇ
        ‚îÇ - Principle of least privilege   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚ùå Attacker blocked here 0.9% of time
                    ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Layer 5: Microservice Logic      ‚îÇ
        ‚îÇ - Input validation               ‚îÇ
        ‚îÇ - Business logic checks          ‚îÇ
        ‚îÇ - Sanitize data                  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚ùå Attacker blocked here 0.09% of time
                    ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Layer 6: Database Access         ‚îÇ
        ‚îÇ - Parameterized queries (no SQL  ‚îÇ
        ‚îÇ   injection)                     ‚îÇ
        ‚îÇ - Encryption at rest             ‚îÇ
        ‚îÇ - Audit logging all access       ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚ùå Attacker blocked here 0.01% of time
                    ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Data Returned to User            ‚îÇ
        ‚îÇ (Only if all layers passed)      ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Probability of breach: 0.01% (very small if all layers working)
Defense strategy: "Defense in depth" - multiple layers
```

---

### Diagram 4: WebSocket Connection Flow with Sticky Session

S∆° ƒë·ªì chi ti·∫øt flow khi client k·∫øt n·ªëi v·ªõi sticky session:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Client 1 (IP: 203.0.113.5)          ‚îÇ
‚îÇ Client 2 (IP: 203.0.113.6)          ‚îÇ
‚îÇ Client 3 (IP: 203.0.113.7)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ Nginx Load        ‚îÇ
         ‚îÇ Balancer          ‚îÇ
         ‚îÇ (IP Hash routing) ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ   ‚îÇ   ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îê ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ hash  ‚îÇ ‚îÇ              ‚îÇ
         ‚îÇ (203..‚îÇ ‚îÇ              ‚îÇ
         ‚îÇ .5)   ‚îÇ ‚îÇ              ‚îÇ
         ‚îÇ ‚Üí     ‚îÇ ‚îÇ              ‚îÇ
         ‚îÇ S1    ‚îÇ ‚îÇ              ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ              ‚îÇ
                   ‚îÇ              ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îê
         ‚îÇ hash(203.0.113.6) ‚Üí S2   ‚îÇ
         ‚îÇ hash(203.0.113.7) ‚Üí S3   ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ       ‚îÇ       ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ Server 1  ‚îÇ ‚îÇ Server2 ‚îÇ ‚îÇ Server 3 ‚îÇ
      ‚îÇ (S1)      ‚îÇ ‚îÇ (S2)    ‚îÇ ‚îÇ (S3)     ‚îÇ
      ‚îÇ           ‚îÇ ‚îÇ         ‚îÇ ‚îÇ          ‚îÇ
      ‚îÇ Client 1  ‚îÇ ‚îÇClient 2 ‚îÇ ‚îÇClient 3  ‚îÇ
      ‚îÇ IP: ...5  ‚îÇ ‚îÇIP: ...6 ‚îÇ ‚îÇIP: ...7  ‚îÇ
      ‚îÇ           ‚îÇ ‚îÇ         ‚îÇ ‚îÇ          ‚îÇ
      ‚îÇ (333      ‚îÇ ‚îÇ (333    ‚îÇ ‚îÇ (334     ‚îÇ
      ‚îÇ  clients) ‚îÇ ‚îÇ  clients)‚îÇ ‚îÇ  clients)‚îÇ
      ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ            ‚îÇ            ‚îÇ
         ‚îÇ Subscribe  ‚îÇ            ‚îÇ
         ‚îÇ to Redis   ‚îÇ            ‚îÇ
         ‚îÇ price-    ‚îÇ             ‚îÇ
         ‚îÇ update ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ channel   ‚îÇ    ‚îÇ
         ‚îÇ           ‚îÇ    ‚îÇ Subscribe
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ Redis Pub/Sub    ‚îÇ
            ‚îÇ Broker           ‚îÇ
            ‚îÇ                  ‚îÇ
            ‚îÇ Channel:         ‚îÇ
            ‚îÇ btcusdt:price    ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îò
                             ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ                             ‚îÇ
              ‚ñº                             ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ Price        ‚îÇ          ‚îÇ Broadcast    ‚îÇ
       ‚îÇ Update: $50k ‚îÇ          ‚îÇ to all 3     ‚îÇ
       ‚îÇ published to ‚îÇ          ‚îÇ servers      ‚îÇ
       ‚îÇ Redis        ‚îÇ          ‚îÇ              ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê
                                      ‚îÇ      ‚îÇ      ‚îÇ
                                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ      ‚îÇ
                                 ‚îÇ Broadcast‚îÇ ‚îÇ      ‚îÇ
                                 ‚îÇto Client1‚îÇ ‚îÇ      ‚îÇ
                                 ‚îÇ(on S1)  ‚îÇ ‚îÇ      ‚îÇ
                                 ‚îÇ         ‚îÇ ‚îÇ      ‚îÇ
                                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ      ‚îÇ
                                             ‚ñº      ‚ñº
                                        Update S2  Update S3
```

Key point: Client lu√¥n "sticky" v·ªõi S1 ‚Üí latency stable (10ms broadcast).

---

### Diagram 5: JWT Token Flow with Refresh Rotation

S∆° ƒë·ªì chi ti·∫øt flow JWT token lifecycle:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ User Login                           ‚îÇ
‚îÇ username: alice                      ‚îÇ
‚îÇ password: ****                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ Auth Service        ‚îÇ
      ‚îÇ                     ‚îÇ
      ‚îÇ 1. Hash password    ‚îÇ
      ‚îÇ 2. Compare with DB  ‚îÇ
      ‚îÇ 3. Match? ‚úì         ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ Generate Tokens                     ‚îÇ
      ‚îÇ                                     ‚îÇ
      ‚îÇ access_token:                       ‚îÇ
      ‚îÇ - Payload: {user_id, role, exp}    ‚îÇ
      ‚îÇ - Expiry: 15 minutes               ‚îÇ
      ‚îÇ - Secret: only server knows        ‚îÇ
      ‚îÇ                                     ‚îÇ
      ‚îÇ refresh_token:                      ‚îÇ
      ‚îÇ - Payload: {user_id, version}      ‚îÇ
      ‚îÇ - Expiry: 7 days                   ‚îÇ
      ‚îÇ - Secret: only server knows        ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Return to client:       ‚îÇ
    ‚îÇ {                       ‚îÇ
    ‚îÇ   access_token: xxx,    ‚îÇ
    ‚îÇ   refresh_token: yyy    ‚îÇ
    ‚îÇ }                       ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
    Client stores in localStorage
                 ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Client calls API                    ‚îÇ
    ‚îÇ GET /user/portfolio                 ‚îÇ
    ‚îÇ Header: Authorization: Bearer xxx   ‚îÇ
    ‚îÇ                                     ‚îÇ
    ‚îÇ (access_token included)             ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ API Gateway verifies token         ‚îÇ
    ‚îÇ                                    ‚îÇ
    ‚îÇ 1. Check signature (valid?)        ‚îÇ
    ‚îÇ 2. Check expiry (not expired?)     ‚îÇ
    ‚îÇ 3. Extract user_id                 ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ Token expired?      ‚îÇ
      ‚îÇ No ‚Üí process request‚îÇ
      ‚îÇ Yes ‚Üí reject 401    ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Client gets 401 Unauthorized      ‚îÇ
    ‚îÇ                                   ‚îÇ
    ‚îÇ ‚Üí Client calls refresh endpoint   ‚îÇ
    ‚îÇ POST /auth/refresh               ‚îÇ
    ‚îÇ Body: {refresh_token: yyy}       ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Auth Service validate            ‚îÇ
    ‚îÇ refresh_token                    ‚îÇ
    ‚îÇ                                  ‚îÇ
    ‚îÇ 1. Verify signature (still good?)‚îÇ
    ‚îÇ 2. Check expiry (not expired?)   ‚îÇ
    ‚îÇ 3. Check blacklist (revoked?)    ‚îÇ
    ‚îÇ 4. All good ‚Üí issue new tokens   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Token Rotation:                    ‚îÇ
    ‚îÇ - Issue new access_token (15 min) ‚îÇ
    ‚îÇ - Issue new refresh_token (7 day) ‚îÇ
    ‚îÇ - Add OLD refresh_token to        ‚îÇ
    ‚îÇ   blacklist (Redis TTL = 7 days)  ‚îÇ
    ‚îÇ                                    ‚îÇ
    ‚îÇ Return new tokens                  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    Client stores new tokens, retry API
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ API call succeed ‚úì     ‚îÇ
    ‚îÇ with new token         ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Timeline:
- T=0: Login ‚Üí get token (valid 15 min)
- T=10: Call API ‚Üí success
- T=20: Call API ‚Üí success
- T=16: Access token expired ‚Üí call refresh
- T=16.5: Get new tokens ‚Üí retry ‚Üí success
- T=100: User logout ‚Üí add token to blacklist
- T=101: Old token invalid (in blacklist) ‚Üí reject
```

---

### Diagram 6: Crawler Hybrid Approach Decision Tree

S∆° ƒë·ªì chi ti·∫øt flow quy·∫øt ƒë·ªãnh extraction trong hybrid crawler:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Fetch Website HTML           ‚îÇ
‚îÇ (e.g., cointelegraph.com)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 1: Try Rule-Based         ‚îÇ
‚îÇ (XPath/CSS selector lookup)    ‚îÇ
‚îÇ                                ‚îÇ
‚îÇ Query: .article-title          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                  ‚îÇ
  Found          Not Found
  (90%)             (10%)
     ‚îÇ                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ Extract text         ‚îÇ‚îÇ
‚îÇ Confidence: 0.95     ‚îÇ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
     ‚îÇ ‚úì Success      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Check confidence level    ‚îÇ
‚îÇ > 70%? ‚Üí return result    ‚îÇ
‚îÇ < 70%? ‚Üí fallback LLM     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
  ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ                                  ‚îÇ
  ‚îÇ ‚úì Confidence ‚â• 70%              ‚îÇ ‚úó Confidence < 70%
  ‚îÇ                                  ‚îÇ OR No match found
  ‚îÇ                                  ‚îÇ
‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Return result         ‚îÇ ‚îÇ Step 2: Try LLM Fallback   ‚îÇ
‚îÇ (Fast path)           ‚îÇ ‚îÇ (OpenAI API)               ‚îÇ
‚îÇ                       ‚îÇ ‚îÇ                             ‚îÇ
‚îÇ Cost: $0              ‚îÇ ‚îÇ Call OpenAI GPT-4           ‚îÇ
‚îÇ Latency: 50ms         ‚îÇ ‚îÇ Prompt: "Extract title,    ‚îÇ
‚îÇ Success: Yes          ‚îÇ ‚îÇ author, date from HTML"    ‚îÇ
‚îÇ                       ‚îÇ ‚îÇ                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ Cost: $0.02                 ‚îÇ
                          ‚îÇ Latency: 30s                ‚îÇ
                          ‚îÇ                             ‚îÇ
                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ               ‚îÇ                ‚îÇ
                    ‚ñº               ‚ñº                ‚ñº
              ‚úì Success      ‚úó Timeout      ‚úó Error/Hallucination
                    ‚îÇ               ‚îÇ                ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ Return LLM  ‚îÇ ‚îÇ Log error  ‚îÇ ‚îÇ Mark "manual  ‚îÇ
            ‚îÇ result      ‚îÇ ‚îÇ Retry in   ‚îÇ ‚îÇ review"       ‚îÇ
            ‚îÇ             ‚îÇ ‚îÇ 5 minutes  ‚îÇ ‚îÇ                ‚îÇ
            ‚îÇ Cost:$0.02  ‚îÇ ‚îÇ            ‚îÇ ‚îÇ Alert ops:     ‚îÇ
            ‚îÇ Latency:30s ‚îÇ ‚îÇ            ‚îÇ ‚îÇ verify needed  ‚îÇ
            ‚îÇ Success:Yes ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Metrics tracked:
- Rule success rate: 90% (so 90% requests fast-path)
- LLM success rate: 85% (so ~13.5% of requests use LLM)
- Manual review rate: 1.5% (so 1.5% need human intervention)

Cost optimization:
- Pure rule-based: $83/month, 60% success
- Pure LLM: $327/month, 92% success
- Hybrid: $132/month, 88% success ‚Üê Best bang for buck
```

---

### Diagram 7: Kubernetes Auto-Scaling Trigger

S∆° ƒë·ªì chi ti·∫øt c√°ch HPA (Horizontal Pod Autoscaler) trigger scale:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Prometheus Metrics Collection        ‚îÇ
‚îÇ (Every 15 seconds)                   ‚îÇ
‚îÇ                                      ‚îÇ
‚îÇ websocket_gateway_cpu: 45%           ‚îÇ
‚îÇ websocket_gateway_memory: 60%        ‚îÇ
‚îÇ websocket_gateway_connections: 5000  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ HPA Evaluation          ‚îÇ
          ‚îÇ (Every 30 seconds)      ‚îÇ
          ‚îÇ                         ‚îÇ
          ‚îÇ CPU > 70%? No           ‚îÇ
          ‚îÇ Memory > 80%? No        ‚îÇ
          ‚îÇ Connections > 10k? No   ‚îÇ
          ‚îÇ                         ‚îÇ
          ‚îÇ ‚Üí No action             ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ Time: 18:45 (peak hour)        ‚îÇ
     ‚îÇ Heavy traffic surge            ‚îÇ
     ‚îÇ                                ‚îÇ
     ‚îÇ websocket_gateway_cpu: 85%     ‚îÇ
     ‚îÇ websocket_gateway_memory: 88%  ‚îÇ
     ‚îÇ websocket_gateway_connections: ‚îÇ
     ‚îÇ 18,000                         ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ HPA Evaluation          ‚îÇ
      ‚îÇ                         ‚îÇ
      ‚îÇ CPU (85%) > 70%? YES ‚úì  ‚îÇ
      ‚îÇ OR Memory (88%) > 80%?  ‚îÇ
      ‚îÇ YES ‚úì                   ‚îÇ
      ‚îÇ                         ‚îÇ
      ‚îÇ ‚Üí TRIGGER SCALE UP      ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ Calculate new replica count:             ‚îÇ
      ‚îÇ                                          ‚îÇ
      ‚îÇ Current: 3 replicas                      ‚îÇ
      ‚îÇ Target CPU: 70%                          ‚îÇ
      ‚îÇ Current CPU: 85%                         ‚îÇ
      ‚îÇ Utilization ratio: 85/70 = 1.21         ‚îÇ
      ‚îÇ                                          ‚îÇ
      ‚îÇ New replicas: 3 √ó 1.21 = 3.64 ‚Üí 4      ‚îÇ
      ‚îÇ (round up to 4)                          ‚îÇ
      ‚îÇ                                          ‚îÇ
      ‚îÇ MaxReplicas: 20 (limit not reached)     ‚îÇ
      ‚îÇ MinReplicas: 3                           ‚îÇ
      ‚îÇ OK to scale up                           ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ Kubernetes Scheduler             ‚îÇ
      ‚îÇ                                  ‚îÇ
      ‚îÇ Create 1 new pod:                ‚îÇ
      ‚îÇ websocket-gateway-4              ‚îÇ
      ‚îÇ                                  ‚îÇ
      ‚îÇ Pull image, init container       ‚îÇ
      ‚îÇ Apply resource limits            ‚îÇ
      ‚îÇ Register with service discovery  ‚îÇ
      ‚îÇ                                  ‚îÇ
      ‚îÇ Time to ready: 10-30 seconds     ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ Result:                          ‚îÇ
      ‚îÇ                                  ‚îÇ
      ‚îÇ Old state: 3 pods √ó 6000 req/s = ‚îÇ
      ‚îÇ            18000 requests        ‚îÇ
      ‚îÇ            CPU = 85%             ‚îÇ
      ‚îÇ                                  ‚îÇ
      ‚îÇ New state: 4 pods √ó 4500 req/s = ‚îÇ
      ‚îÇ            18000 requests        ‚îÇ
      ‚îÇ            CPU = 64%             ‚îÇ
      ‚îÇ                                  ‚îÇ
      ‚îÇ ‚úì CPU back to acceptable level   ‚îÇ
      ‚îÇ ‚úì Users experience better latency‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Timeline:
- T=18:45:00 - CPU spike to 85%, HPA detection
- T=18:45:30 - HPA calculate & create new pod
- T=18:45:45 - Pod initializing (pull image ~15s)
- T=18:46:00 - Pod ready, traffic routing start
- T=18:46:15 - All traffic rebalanced, CPU ‚Üí 64%

Note: Scaling down happen similarly when traffic decrease:
- CPU < 50% for 3 minutes
- HPA scale down: remove 1 pod
- New state: 3 pods, less cost
```

---

### File References & Links

**PlantUML source files (trong `diagrams/` folder):**

- `01_system_overview.puml` - T·ªïng quan ki·∫øn tr√∫c h·ªá th·ªëng
- `02_ten_layers.puml` - 10 layers security + infrastructure
- `06_deployment_k8s.puml` - Kubernetes deployment manifests
- `07_problem1_websocket_scaling.puml` - WebSocket scaling solution comparison
- `08_problem2_crawler_recovery.puml` - Crawler recovery mechanism

**Generated PNG files (trong `diagrams_images/` folder):**

- `01_system_overview.puml.png` - Rendered system overview
- `02_ten_layers.png` - Rendered 10 layers
- `07_problem1_websocket_scaling.png` - Rendered WebSocket comparison
- `08_problem2_crawler_recovery.png` - Rendered crawler flow

ƒê·ªÉ render PlantUML th√†nh PNG:

```bash
# Install PlantUML locally
sudo apt-get install plantuml

# Or use online renderer
# https://www.planttext.com/
# Paste .puml file content ‚Üí generate PNG ‚Üí download
```

---

## FINAL CHECKLIST

‚úÖ **Content**

- 1761+ d√≤ng ti·∫øng Vi·ªát 100%
- 3 scenarios chi ti·∫øt (WebSocket, Crawler, Security)
- 4-5 l√Ω do "T·∫†I SAO PH·∫¢I D√ôNG" cho m·ªói gi·∫£i ph√°p
- So s√°nh trade-off r√µ r√†ng (cost, latency, complexity, scalability)
- Metrics chi ti·∫øt (P99 latency, success rate, cost/th√°ng)

‚úÖ **Diagrams**

- 13 image embeds t√≠ch h·ª£p (PNG t·ª´ diagrams_images/)
- 7 detailed ASCII diagrams trong Appendix
- Reference ƒë·∫ßy ƒë·ªß t·ªõi PlantUML source files

‚úÖ **Implementation**

- Phase-by-phase guide (Week 1-8)
- Docker Compose setup for local dev
- Kubernetes manifests for production
- Monitoring & alerting configuration
- Troubleshooting guide (5 common issues)

‚úÖ **Production Ready**

- Cost analysis & ROI calculation
- Risk matrix & mitigation strategies
- Timeline & resource planning
- Real-world case study (deployment, incidents, optimization)
- Self-assessment questions

---
